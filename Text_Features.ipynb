{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook illustrates the translation of complex objects (plain text documents) into a set of features suitable for training a machine learning algorithm.\n",
    "\n",
    "We will use the simplest of such models the **Bag of Words** on the [Reuters 50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50) collection of texts.\n",
    "\n",
    "**Note** If the original link to the source data above is down. You can get a google drive copy of the C50 Reuters dataset from [here](https://drive.google.com/drive/folders/1kKFrHulkbxknPDEp9A1f1KwIYzyfcDME?usp=sharing)\n",
    "\n",
    "In this notebook we will:\n",
    "1. Discuss a pipeline to transform files into list of words.\n",
    "2. Implement a few document similarity metrics.\n",
    "3. Compare `sklearn` document vectorizer implementations to our own.\n",
    "4.  Save some preprocessed data sets for later reuse.\n",
    "\n",
    "In practice we will want to use `sklearn` vectorizers, but here we re-implement them ourselves so that we understand all the subtle details of their definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:58:43.068585Z",
     "start_time": "2019-09-19T16:58:42.339933Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "# Domain specific libraries to handle text\n",
    "#\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fist time Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook downloads some data, and generates some pre-processed files we will neede later.\n",
    "\n",
    "Set the `fist_time` flag below to True **once**, after that, it will be faster to run the notebook\n",
    "with the flag set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:58:43.072576Z",
     "start_time": "2019-09-19T16:58:43.070053Z"
    }
   },
   "outputs": [],
   "source": [
    "first_time=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean_text/AGTC_DEF 14A_20181115_0001193125-18...</td>\n",
       "      <td>AGTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clean_text/AHPI_DEF 14A_20181108_0001144204-18...</td>\n",
       "      <td>AHPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean_text/AKRX_DEF 14A_20181121_0001308179-18...</td>\n",
       "      <td>AKRX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clean_text/AMTY_DEF 14A_20181128_0001185185-18...</td>\n",
       "      <td>AMTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clean_text/AOSL_DEF 14A_20181109_0001387467-18...</td>\n",
       "      <td>AOSL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     file_name label\n",
       "document_id                                                         \n",
       "0            clean_text/AGTC_DEF 14A_20181115_0001193125-18...  AGTC\n",
       "1            clean_text/AHPI_DEF 14A_20181108_0001144204-18...  AHPI\n",
       "2            clean_text/AKRX_DEF 14A_20181121_0001308179-18...  AKRX\n",
       "3            clean_text/AMTY_DEF 14A_20181128_0001185185-18...  AMTY\n",
       "4            clean_text/AOSL_DEF 14A_20181109_0001387467-18...  AOSL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=pd.read_csv(\"/Users/Emily/Desktop/dfg-humanrights/filenames.csv\",index_col=\"document_id\")\n",
    "documents['file_name']=documents['file_name'].apply(lambda x: \"clean_text/\"+ x )\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Set Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because documents can be large  we will just keep a list of their file names in memory.\n",
    "\n",
    "We will try really hard not read them all into memory at the same time.\n",
    "\n",
    "Each document is labeled by its author.  We will  use that label later on the course, but not today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag Of Words Document Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest models of a document is the **Bag of Words** model:\n",
    "\n",
    "we **ignore word ordering** and represent a\n",
    "document by the list of words that it containts.\n",
    "\n",
    "We still have a lot of choices we could make:\n",
    "\n",
    "* Does punctuation like `.` and `?` count as words or do we ignore them?\n",
    "* is `New York` one word or two, what about `U.S.`?\n",
    "* Do `car` and `cars` count as different words?\n",
    "* What about `safe` and `safely`\n",
    "* What do we do about miss-spelled words, do we try to fix them?\n",
    "* Do we consider different capitalizations of the same word:  `Car` versus `car`?\n",
    "* Do we include high frequency, low information content words such as `a` and `the` in our bag of words?\n",
    "\n",
    "All this choices are **problem dependent**. If we have a particular ML problem to solve we will use our **domain** knowledge to resolve those questions in the context of that specific task.\n",
    "\n",
    "Today we just illustrate how to put together a **data pre-processing** pipeline. \n",
    "\n",
    "The pipeline is designed in such a way that it will be easier later to change our answer to any of those questions and control how we prepare the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use a default pipeline that is both sensible and simple to implement:\n",
    "* We remove punctuation\n",
    "* `New York` counts as two words but  `U.S` counts as one.\n",
    "* `car` and \"`cars` are the same word, same for `safe`, and `safely`.\n",
    "* miss-spelled words count as different words.\n",
    "* We remove capitalization so `Car` and `car` count as the same word.\n",
    "* We remove high frequency such as `a` and `the`.\n",
    "\n",
    "We will rely on python's **NLTK** (Natural Language Toolkit) library to perform tasks that require **domain** knowledge about\n",
    "text processing:\n",
    "* **tokenization**: breaking character streams into words\n",
    "* **stemming**: normalizing words into their roots: `cars` -> `car`, `safely` -> `safe`\n",
    "* **stop word removal**: high frequency, low information words that we will consider just *noise* and ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function that goes from text to stems for reuse later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:58:43.872078Z",
     "start_time": "2019-09-19T16:58:43.867816Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem_tokenizer(text):\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    return [porter_stemmer.stem(token) for token in word_tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to run the stop words through stemmer (so that they will we identified and removed from text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unit',\n",
       " 'state',\n",
       " 'secur',\n",
       " 'and',\n",
       " 'exchang',\n",
       " 'commiss',\n",
       " 'washington',\n",
       " ',',\n",
       " 'd.c.',\n",
       " '20549',\n",
       " 'schedul',\n",
       " '14a',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'pursuant',\n",
       " 'to',\n",
       " 'section',\n",
       " '14',\n",
       " '(',\n",
       " 'a',\n",
       " ')',\n",
       " 'of',\n",
       " 'the',\n",
       " 'secur',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'of',\n",
       " '1934',\n",
       " 'file',\n",
       " 'by',\n",
       " 'the',\n",
       " 'registr',\n",
       " '☒',\n",
       " 'file',\n",
       " 'by',\n",
       " 'a',\n",
       " 'parti',\n",
       " 'other',\n",
       " 'than',\n",
       " 'the',\n",
       " 'registr',\n",
       " '☐',\n",
       " 'check',\n",
       " 'the',\n",
       " 'appropri',\n",
       " 'box',\n",
       " ':',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " '☐',\n",
       " 'preliminari',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " '☐',\n",
       " 'confidenti',\n",
       " ',',\n",
       " 'for',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'commiss',\n",
       " 'onli',\n",
       " '(',\n",
       " 'as',\n",
       " 'permit',\n",
       " 'by',\n",
       " 'rule',\n",
       " '14a-6',\n",
       " '(',\n",
       " 'e',\n",
       " ')',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " ')',\n",
       " '☒',\n",
       " 'definit',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " '☐',\n",
       " 'definit',\n",
       " 'addit',\n",
       " 'materi',\n",
       " '☐',\n",
       " 'solicit',\n",
       " 'materi',\n",
       " 'pursuant',\n",
       " 'to',\n",
       " '§240.14a-12',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'appli',\n",
       " 'genet',\n",
       " 'technolog',\n",
       " 'corpor',\n",
       " '(',\n",
       " 'name',\n",
       " 'of',\n",
       " 'registr',\n",
       " 'as',\n",
       " 'specifi',\n",
       " 'in',\n",
       " 'it',\n",
       " 'charter',\n",
       " ')',\n",
       " 'not',\n",
       " 'applic',\n",
       " '(',\n",
       " 'name',\n",
       " 'of',\n",
       " 'person',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " 'file',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " ',',\n",
       " 'if',\n",
       " 'other',\n",
       " 'than',\n",
       " 'the',\n",
       " 'registr',\n",
       " ')',\n",
       " 'payment',\n",
       " 'of',\n",
       " 'file',\n",
       " 'fee',\n",
       " '(',\n",
       " 'check',\n",
       " 'the',\n",
       " 'appropri',\n",
       " 'box',\n",
       " ')',\n",
       " ':',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " '☒',\n",
       " 'no',\n",
       " 'fee',\n",
       " 'requir',\n",
       " '.',\n",
       " '☐',\n",
       " 'fee',\n",
       " 'comput',\n",
       " 'on',\n",
       " 'tabl',\n",
       " 'below',\n",
       " 'per',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '14a-6',\n",
       " '(',\n",
       " 'i',\n",
       " ')',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'and',\n",
       " '0-11',\n",
       " '.',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'titl',\n",
       " 'of',\n",
       " 'each',\n",
       " 'class',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'titl',\n",
       " 'of',\n",
       " 'each',\n",
       " 'class',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'titl',\n",
       " 'of',\n",
       " 'each',\n",
       " 'class',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'aggreg',\n",
       " 'number',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'aggreg',\n",
       " 'number',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'aggreg',\n",
       " 'number',\n",
       " 'of',\n",
       " 'secur',\n",
       " 'to',\n",
       " 'which',\n",
       " 'transact',\n",
       " 'appli',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'per',\n",
       " 'unit',\n",
       " 'price',\n",
       " 'or',\n",
       " 'other',\n",
       " 'underli',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " 'comput',\n",
       " 'pursuant',\n",
       " 'to',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " '(',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'on',\n",
       " 'which',\n",
       " 'the',\n",
       " 'file',\n",
       " 'fee',\n",
       " 'is',\n",
       " 'calcul',\n",
       " 'and',\n",
       " 'state',\n",
       " 'how',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'determin',\n",
       " ')',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'per',\n",
       " 'unit',\n",
       " 'price',\n",
       " 'or',\n",
       " 'other',\n",
       " 'underli',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " 'comput',\n",
       " 'pursuant',\n",
       " 'to',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " '(',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'on',\n",
       " 'which',\n",
       " 'the',\n",
       " 'file',\n",
       " 'fee',\n",
       " 'is',\n",
       " 'calcul',\n",
       " 'and',\n",
       " 'state',\n",
       " 'how',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'determin',\n",
       " ')',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'per',\n",
       " 'unit',\n",
       " 'price',\n",
       " 'or',\n",
       " 'other',\n",
       " 'underli',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " 'comput',\n",
       " 'pursuant',\n",
       " 'to',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " '(',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'on',\n",
       " 'which',\n",
       " 'the',\n",
       " 'file',\n",
       " 'fee',\n",
       " 'is',\n",
       " 'calcul',\n",
       " 'and',\n",
       " 'state',\n",
       " 'how',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'determin',\n",
       " ')',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'propos',\n",
       " 'maximum',\n",
       " 'aggreg',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'propos',\n",
       " 'maximum',\n",
       " 'aggreg',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'propos',\n",
       " 'maximum',\n",
       " 'aggreg',\n",
       " 'valu',\n",
       " 'of',\n",
       " 'transact',\n",
       " ':',\n",
       " '(',\n",
       " '5',\n",
       " ')',\n",
       " 'total',\n",
       " 'fee',\n",
       " 'paid',\n",
       " ':',\n",
       " '(',\n",
       " '5',\n",
       " ')',\n",
       " 'total',\n",
       " 'fee',\n",
       " 'paid',\n",
       " ':',\n",
       " '(',\n",
       " '5',\n",
       " ')',\n",
       " 'total',\n",
       " 'fee',\n",
       " 'paid',\n",
       " ':',\n",
       " '☐',\n",
       " 'fee',\n",
       " 'paid',\n",
       " 'previous',\n",
       " 'with',\n",
       " 'preliminari',\n",
       " 'materi',\n",
       " '.',\n",
       " '☐',\n",
       " 'check',\n",
       " 'box',\n",
       " 'if',\n",
       " 'ani',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fee',\n",
       " 'is',\n",
       " 'offset',\n",
       " 'as',\n",
       " 'provid',\n",
       " 'by',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " '(',\n",
       " 'a',\n",
       " ')',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'and',\n",
       " 'identifi',\n",
       " 'the',\n",
       " 'file',\n",
       " 'for',\n",
       " 'which',\n",
       " 'the',\n",
       " 'offset',\n",
       " 'fee',\n",
       " 'wa',\n",
       " 'paid',\n",
       " 'previous',\n",
       " '.',\n",
       " 'identifi',\n",
       " 'the',\n",
       " 'previou',\n",
       " 'file',\n",
       " 'by',\n",
       " 'registr',\n",
       " 'statement',\n",
       " 'number',\n",
       " ',',\n",
       " 'or',\n",
       " 'the',\n",
       " 'form',\n",
       " 'or',\n",
       " 'schedul',\n",
       " 'and',\n",
       " 'the',\n",
       " 'date',\n",
       " 'of',\n",
       " 'it',\n",
       " 'file',\n",
       " '.',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'amount',\n",
       " 'previous',\n",
       " 'paid',\n",
       " ':',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'amount',\n",
       " 'previous',\n",
       " 'paid',\n",
       " ':',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'amount',\n",
       " 'previous',\n",
       " 'paid',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'form',\n",
       " ',',\n",
       " 'schedul',\n",
       " 'or',\n",
       " 'registr',\n",
       " 'statement',\n",
       " 'no',\n",
       " '.',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'form',\n",
       " ',',\n",
       " 'schedul',\n",
       " 'or',\n",
       " 'registr',\n",
       " 'statement',\n",
       " 'no',\n",
       " '.',\n",
       " ':',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'form',\n",
       " ',',\n",
       " 'schedul',\n",
       " 'or',\n",
       " 'registr',\n",
       " 'statement',\n",
       " 'no',\n",
       " '.',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'file',\n",
       " 'parti',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'file',\n",
       " 'parti',\n",
       " ':',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'file',\n",
       " 'parti',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'date',\n",
       " 'file',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'date',\n",
       " 'file',\n",
       " ':',\n",
       " '(',\n",
       " '4',\n",
       " ')',\n",
       " 'date',\n",
       " 'file',\n",
       " ':',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'appli',\n",
       " 'genet',\n",
       " 'technolog',\n",
       " 'corpor',\n",
       " '14193',\n",
       " 'nw',\n",
       " '119th',\n",
       " 'terrac',\n",
       " 'alachua',\n",
       " ',',\n",
       " 'florida',\n",
       " '32615',\n",
       " 'notic',\n",
       " 'of',\n",
       " '2019',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'of',\n",
       " 'stockhold',\n",
       " 'dear',\n",
       " 'stockhold',\n",
       " ':',\n",
       " 'we',\n",
       " 'invit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'attend',\n",
       " 'our',\n",
       " '2019',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'of',\n",
       " 'stockhold',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'be',\n",
       " 'held',\n",
       " 'as',\n",
       " 'follow',\n",
       " ':',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'date',\n",
       " ':',\n",
       " 'novemb',\n",
       " '15',\n",
       " ',',\n",
       " '2018',\n",
       " 'time',\n",
       " ':',\n",
       " '2:00',\n",
       " 'p.m.',\n",
       " ',',\n",
       " 'eastern',\n",
       " 'time',\n",
       " 'locat',\n",
       " ':',\n",
       " 'appli',\n",
       " 'genet',\n",
       " 'technolog',\n",
       " 'corpor',\n",
       " 'one',\n",
       " 'kendal',\n",
       " 'squar',\n",
       " ',',\n",
       " '1400w',\n",
       " 'cambridg',\n",
       " ',',\n",
       " 'ma',\n",
       " '02139',\n",
       " 'locat',\n",
       " ':',\n",
       " 'appli',\n",
       " 'genet',\n",
       " 'technolog',\n",
       " 'corpor',\n",
       " 'one',\n",
       " 'kendal',\n",
       " 'squar',\n",
       " ',',\n",
       " '1400w',\n",
       " 'cambridg',\n",
       " ',',\n",
       " 'ma',\n",
       " '02139',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'at',\n",
       " 'the',\n",
       " 'meet',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'ask',\n",
       " 'our',\n",
       " 'stockhold',\n",
       " 'to',\n",
       " ':',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " '•',\n",
       " 're-elect',\n",
       " 'as',\n",
       " 'our',\n",
       " 'class',\n",
       " 'ii',\n",
       " 'director',\n",
       " 'scott',\n",
       " 'koenig',\n",
       " 'and',\n",
       " 'ivana',\n",
       " 'magovcevic-liebisch',\n",
       " ',',\n",
       " 'each',\n",
       " 'to',\n",
       " 'serv',\n",
       " 'for',\n",
       " 'a',\n",
       " 'three-year',\n",
       " 'term',\n",
       " 'end',\n",
       " 'at',\n",
       " 'our',\n",
       " '2022',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'of',\n",
       " 'stockhold',\n",
       " ';',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " '•',\n",
       " 'ratifi',\n",
       " 'the',\n",
       " 'appoint',\n",
       " 'of',\n",
       " 'ernst',\n",
       " '&',\n",
       " 'young',\n",
       " 'llp',\n",
       " 'as',\n",
       " 'our',\n",
       " 'independ',\n",
       " 'regist',\n",
       " 'public',\n",
       " 'account',\n",
       " 'firm',\n",
       " 'for',\n",
       " 'fiscal',\n",
       " 'year',\n",
       " '2019',\n",
       " ';',\n",
       " 'and',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " '•',\n",
       " 'consid',\n",
       " 'ani',\n",
       " 'other',\n",
       " 'busi',\n",
       " 'properli',\n",
       " 'present',\n",
       " 'at',\n",
       " 'the',\n",
       " 'meet',\n",
       " '.',\n",
       " '[',\n",
       " 'end',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'you',\n",
       " 'may',\n",
       " 'vote',\n",
       " 'on',\n",
       " 'these',\n",
       " 'matter',\n",
       " 'in',\n",
       " 'person',\n",
       " ',',\n",
       " 'by',\n",
       " 'proxi',\n",
       " 'or',\n",
       " 'via',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'or',\n",
       " 'telephon',\n",
       " '.',\n",
       " 'whether',\n",
       " 'or',\n",
       " 'not',\n",
       " 'you',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'attend',\n",
       " 'the',\n",
       " 'meet',\n",
       " ',',\n",
       " 'we',\n",
       " 'ask',\n",
       " 'that',\n",
       " 'you',\n",
       " 'promptli',\n",
       " 'complet',\n",
       " 'and',\n",
       " 'return',\n",
       " 'the',\n",
       " 'enclos',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'in',\n",
       " 'the',\n",
       " 'enclos',\n",
       " 'address',\n",
       " ',',\n",
       " 'postage-paid',\n",
       " 'envelop',\n",
       " 'or',\n",
       " 'vote',\n",
       " 'via',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'or',\n",
       " 'telephon',\n",
       " ',',\n",
       " 'so',\n",
       " 'that',\n",
       " 'your',\n",
       " 'share',\n",
       " 'will',\n",
       " 'be',\n",
       " 'repres',\n",
       " 'and',\n",
       " 'vote',\n",
       " 'at',\n",
       " 'the',\n",
       " 'meet',\n",
       " 'in',\n",
       " 'accord',\n",
       " 'with',\n",
       " 'your',\n",
       " 'wish',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'attend',\n",
       " 'the',\n",
       " 'meet',\n",
       " ',',\n",
       " 'you',\n",
       " 'may',\n",
       " 'withdraw',\n",
       " 'your',\n",
       " 'proxi',\n",
       " 'or',\n",
       " 'internet',\n",
       " 'or',\n",
       " 'telephon',\n",
       " 'vote',\n",
       " 'and',\n",
       " 'vote',\n",
       " 'your',\n",
       " 'share',\n",
       " 'in',\n",
       " 'person',\n",
       " '.',\n",
       " 'onli',\n",
       " 'stockhold',\n",
       " 'of',\n",
       " 'record',\n",
       " 'at',\n",
       " 'the',\n",
       " 'close',\n",
       " 'of',\n",
       " 'busi',\n",
       " 'on',\n",
       " 'octob',\n",
       " '15',\n",
       " ',',\n",
       " '2018',\n",
       " 'may',\n",
       " 'vote',\n",
       " 'at',\n",
       " 'the',\n",
       " 'meet',\n",
       " '.',\n",
       " 'by',\n",
       " 'order',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'director',\n",
       " ',',\n",
       " 'hemmi',\n",
       " 'chang',\n",
       " 'secretari',\n",
       " 'octob',\n",
       " '16',\n",
       " ',',\n",
       " '2018',\n",
       " '*****************',\n",
       " 'import',\n",
       " 'notic',\n",
       " 'regard',\n",
       " 'the',\n",
       " 'avail',\n",
       " 'of',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'for',\n",
       " 'the',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'of',\n",
       " 'stockhold',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held',\n",
       " 'on',\n",
       " 'novemb',\n",
       " '15',\n",
       " ',',\n",
       " '2018',\n",
       " 'thi',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'and',\n",
       " 'our',\n",
       " 'fiscal',\n",
       " 'year',\n",
       " '2018',\n",
       " 'annual',\n",
       " 'report',\n",
       " 'to',\n",
       " 'stockhold',\n",
       " 'are',\n",
       " 'also',\n",
       " 'avail',\n",
       " 'for',\n",
       " 'view',\n",
       " ',',\n",
       " 'print',\n",
       " 'and',\n",
       " 'download',\n",
       " 'at',\n",
       " 'www.edocumentview.com/agtc',\n",
       " '.',\n",
       " 'i',\n",
       " 'tabl',\n",
       " 'of',\n",
       " 'content',\n",
       " '[',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " ']',\n",
       " 'inform',\n",
       " 'about',\n",
       " 'the',\n",
       " 'meet',\n",
       " '1',\n",
       " 'the',\n",
       " 'meet',\n",
       " '1',\n",
       " 'thi',\n",
       " 'proxi',\n",
       " 'solicit',\n",
       " '1',\n",
       " 'who',\n",
       " 'may',\n",
       " 'vote',\n",
       " '1',\n",
       " 'how',\n",
       " 'to',\n",
       " 'vote',\n",
       " '1',\n",
       " 'share',\n",
       " 'held',\n",
       " 'by',\n",
       " 'broker',\n",
       " 'or',\n",
       " 'nomine',\n",
       " '2',\n",
       " 'quorum',\n",
       " 'requir',\n",
       " 'to',\n",
       " 'transact',\n",
       " 'busi',\n",
       " '3',\n",
       " 'multipl',\n",
       " 'stockhold',\n",
       " 'share',\n",
       " 'the',\n",
       " 'same',\n",
       " 'address',\n",
       " '3',\n",
       " 'propos',\n",
       " '1',\n",
       " ':',\n",
       " 'elect',\n",
       " 'of',\n",
       " 'director',\n",
       " '4',\n",
       " 'propos',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(documents['file_name'].loc[0], \"r\")\n",
    "process=stem_tokenizer(f.read())\n",
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(filename,stop): \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    file=open(filename)\n",
    "    lines=file.readlines()\n",
    "    text=\" \".join(lines).replace(\"\\n\",\" \").replace('\\u200b', '').replace('\\n', '')\n",
    "    stem_list=stem_tokenizer(text)\n",
    "    used_list=[token for token in stem_list if token not in stop_words]\n",
    "    #used_list.remove('\\u200b')\n",
    "    return used_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=documents[\"file_name\"][10]\n",
    "punctuation=list(string.punctuation)\n",
    "stop0=stopwords.words(\"english\")+punctuation\n",
    "stop_words=set(stem_tokenizer(\" \".join(stop0)))\n",
    "stems=process_text(filename,stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPPED HEREEEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean_text/AGTC_DEF 14A_20181115_0001193125-18...</td>\n",
       "      <td>AGTC</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clean_text/AHPI_DEF 14A_20181108_0001144204-18...</td>\n",
       "      <td>AHPI</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean_text/AKRX_DEF 14A_20181121_0001308179-18...</td>\n",
       "      <td>AKRX</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clean_text/AMTY_DEF 14A_20181128_0001185185-18...</td>\n",
       "      <td>AMTY</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clean_text/AOSL_DEF 14A_20181109_0001387467-18...</td>\n",
       "      <td>AOSL</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clean_text/APD_DEF 14A_20190124_0001206774-18-...</td>\n",
       "      <td>APD</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clean_text/ARAY_DEF 14A_20181116_0001193125-18...</td>\n",
       "      <td>ARAY</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clean_text/ARCW_DEF 14A_20181204_0001558370-18...</td>\n",
       "      <td>ARCW</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clean_text/ARL_DEF 14A_20181212_0001387131-18-...</td>\n",
       "      <td>ARL</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clean_text/AYI_DEF 14A_20180831_0001144215-18-...</td>\n",
       "      <td>AYI</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clean_text/DIT_DEF 14A_20181116_0001014108-18-...</td>\n",
       "      <td>DIT</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clean_text/EPAC_DEF 14A_20180831_0000006955-18...</td>\n",
       "      <td>EPAC</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clean_text/FLWS_DEF 14A_20181211_0001437749-18...</td>\n",
       "      <td>FLWS</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20181018_0001104659-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20181025_0001437749-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20181128_0000051931-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20181128_0000051931-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20181129_0001213900-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20190124_0001387131-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20190124_0001398344-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>clean_text/_UNK_DEF 14A_20190213_0001398344-18...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>[unit, state, secur, exchang, commiss, washing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     file_name label  \\\n",
       "document_id                                                            \n",
       "0            clean_text/AGTC_DEF 14A_20181115_0001193125-18...  AGTC   \n",
       "1            clean_text/AHPI_DEF 14A_20181108_0001144204-18...  AHPI   \n",
       "2            clean_text/AKRX_DEF 14A_20181121_0001308179-18...  AKRX   \n",
       "3            clean_text/AMTY_DEF 14A_20181128_0001185185-18...  AMTY   \n",
       "4            clean_text/AOSL_DEF 14A_20181109_0001387467-18...  AOSL   \n",
       "5            clean_text/APD_DEF 14A_20190124_0001206774-18-...   APD   \n",
       "6            clean_text/ARAY_DEF 14A_20181116_0001193125-18...  ARAY   \n",
       "7            clean_text/ARCW_DEF 14A_20181204_0001558370-18...  ARCW   \n",
       "8            clean_text/ARL_DEF 14A_20181212_0001387131-18-...   ARL   \n",
       "9            clean_text/AYI_DEF 14A_20180831_0001144215-18-...   AYI   \n",
       "10           clean_text/DIT_DEF 14A_20181116_0001014108-18-...   DIT   \n",
       "11           clean_text/EPAC_DEF 14A_20180831_0000006955-18...  EPAC   \n",
       "12           clean_text/FLWS_DEF 14A_20181211_0001437749-18...  FLWS   \n",
       "13           clean_text/_UNK_DEF 14A_20181018_0001104659-18...   UNK   \n",
       "14           clean_text/_UNK_DEF 14A_20181025_0001437749-18...   UNK   \n",
       "15           clean_text/_UNK_DEF 14A_20181128_0000051931-18...   UNK   \n",
       "16           clean_text/_UNK_DEF 14A_20181128_0000051931-18...   UNK   \n",
       "17           clean_text/_UNK_DEF 14A_20181129_0001213900-18...   UNK   \n",
       "18           clean_text/_UNK_DEF 14A_20190124_0001387131-18...   UNK   \n",
       "19           clean_text/_UNK_DEF 14A_20190124_0001398344-18...   UNK   \n",
       "20           clean_text/_UNK_DEF 14A_20190213_0001398344-18...   UNK   \n",
       "\n",
       "                                                    clean_text  \n",
       "document_id                                                     \n",
       "0            [unit, state, secur, exchang, commiss, washing...  \n",
       "1            [unit, state, secur, exchang, commiss, washing...  \n",
       "2            [unit, state, secur, exchang, commiss, washing...  \n",
       "3            [unit, state, secur, exchang, commiss, washing...  \n",
       "4            [unit, state, secur, exchang, commiss, washing...  \n",
       "5            [unit, state, secur, exchang, commiss, washing...  \n",
       "6            [unit, state, secur, exchang, commiss, washing...  \n",
       "7            [unit, state, secur, exchang, commiss, washing...  \n",
       "8            [unit, state, secur, exchang, commiss, washing...  \n",
       "9            [unit, state, secur, exchang, commiss, washing...  \n",
       "10           [unit, state, secur, exchang, commiss, washing...  \n",
       "11           [unit, state, secur, exchang, commiss, washing...  \n",
       "12           [unit, state, secur, exchang, commiss, washing...  \n",
       "13           [unit, state, secur, exchang, commiss, washing...  \n",
       "14           [unit, state, secur, exchang, commiss, washing...  \n",
       "15           [unit, state, secur, exchang, commiss, washing...  \n",
       "16           [unit, state, secur, exchang, commiss, washing...  \n",
       "17           [unit, state, secur, exchang, commiss, washing...  \n",
       "18           [unit, state, secur, exchang, commiss, washing...  \n",
       "19           [unit, state, secur, exchang, commiss, washing...  \n",
       "20           [unit, state, secur, exchang, commiss, washing...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['clean_text']=documents['file_name'].apply(lambda x: process_text(filename, stop_words))\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a representation of a document as a stream of tokes (stems), we need to define the concept of a document distance metric.\n",
    "\n",
    "Usually in text processing the concept described is the normalized similarity $0<s(t_1,t_2)<1$, the translation to distance is simply\n",
    "\n",
    "$$\n",
    "    d(t_1,t_2) = 1-s(t_1,t_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, may different definitions of similarity are possible, we will consider three here:\n",
    "* Set intersection similarity.\n",
    "* vector of counts similarity.\n",
    "* TF-IDF (Term Frequency, Inverse Document Frequency) similarity.\n",
    "\n",
    "There are many more choices, and, as usual, which one works best depends on the  problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Intersection Similarity Measure\n",
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider our **bag of words** as just the set with the stems contained in document.\n",
    "\n",
    "Then document similarity between to documetns is just the normalized intersection of  the document's stem's sets.\n",
    "\n",
    "$$\n",
    "    S_{\\textrm{set}}(S_1,S_2)= \\frac{|S_1 \\cap S_2|}{\\sqrt{|S_1|\\cdot |S_2|}}\n",
    "$$\n",
    "where $|\\cdot|$ is the set cardinality (number of elements).\n",
    "\n",
    "In words: the *set similarity* of two documents is the ratio of the number of words in common to the geometric mean of the document's vocabulary length.\n",
    "\n",
    "This is the same as considering $S_i$ as a **one-hot-encoded** vector of words: each word in the vocabulary is a dimension, and a component of the vector is 1 if that word is present on the document, 0 otherwise.  \n",
    "\n",
    "With that interpretation\n",
    "$$\n",
    "    S_{\\textrm{set}}(S_1,S_2)= \\frac{S_1 \\cdot S_2}{\\sqrt{|S_1|\\cdot |S_2|}}=\\cos(S_1,S_2)\n",
    "$$\n",
    "where $\\cdot$ is the regular scalar product of vectors and $|\\cdot|$ is the vector norm (numerically identical to the set's cardinality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how documents from the same author are more similar to each other than to documents from a second author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Similarity Measure\n",
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include a bit more information on the **Bag of Words** model by keep track on how many times each word appear on a document\n",
    "$$\n",
    "    S_{\\textrm{count}} = \\frac{C_1 \\cdot C_2}{\\sqrt{|C_1|\\cdot |C_2|}}=\\cos(C_1,C_2)\n",
    "$$\n",
    "This is the same cosine similarity used on the set case but a  **count's feature vector** rather than been only 0 or 1, each dimension $w$ of document  $C$ contains the number times  word $w$ appears in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "`python` collections module has a [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) object that computes the counts for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, documents from the same author are closer to each other. But now the differences look more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction with `klearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` provide combinient methods to import text into a set of features suitable for machine learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Text as Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure a `CountVectorizer` that will\n",
    "* use a list of `filename`s as input and read the file to get text\n",
    "* process text using `steam_tokenizer`\n",
    "* remove stop words from our `stop` list\n",
    "\n",
    "It will return a matrix of word counts arranged by `(document index)` x (`word index`)\n",
    "\n",
    "each word is mapped to its own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:59:14.126457Z",
     "start_time": "2019-09-19T16:59:14.124212Z"
    }
   },
   "outputs": [],
   "source": [
    "countVectorizer=CountVectorizer(input=\"filename\",tokenizer=stem_tokenizer,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a count vectorizer, but we still need to learn a vocabulary by **fitting** a corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:59:37.114173Z",
     "start_time": "2019-09-19T16:59:14.128481Z"
    }
   },
   "outputs": [],
   "source": [
    "X=countVectorizer.fit_transform(documents[\"file_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a **sparse matrix**, it has 2,500 rows, one per document, and 28,131 columns, one for each word in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:59:37.132156Z",
     "start_time": "2019-09-19T16:59:37.119643Z"
    }
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First document had 197 words, so only 197 columns out of 28131 are non-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know what columns have positive counts we use the `nonzero` function that returns  (row,colum) for nonzero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:59:37.140417Z",
     "start_time": "2019-09-19T16:59:37.133409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7921, 7636, 1195, ..., 7817, 8093, 8615], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].nonzero()[1] # we only want the column, we already know row is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets count the **total number of words in corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:59:37.146751Z",
     "start_time": "2019-09-19T16:59:37.142056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words 297963\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Words\",X.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So X is just a **matrix**:\n",
    "* **rows** are **documents**\n",
    "* **columns** are **words**, each word has its own column\n",
    "\n",
    "There are 28k columns, but the representation is **sparse** to save memory, only non-zero entries are stored in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Text as a Set of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To represent text as a set of words (instead of a vector of counts) we pass an extra flag to\n",
    "`CountVectorizer` so that it only counts up to `one`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:00.659653Z",
     "start_time": "2019-09-19T17:00:00.641549Z"
    }
   },
   "outputs": [],
   "source": [
    "setVectorizer=CountVectorizer(input=\"filename\",binary=True,tokenizer=stem_tokenizer,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:23.808376Z",
     "start_time": "2019-09-19T17:00:00.660941Z"
    }
   },
   "outputs": [],
   "source": [
    "X_set=setVectorizer.fit_transform(documents[\"file_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Text as TF-IDF weighted Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` has also a vectorizer that weights columns by the inverse document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0001</th>\n",
       "      <th>''</th>\n",
       "      <th>'at</th>\n",
       "      <th>'execut</th>\n",
       "      <th>'for</th>\n",
       "      <th>'reason</th>\n",
       "      <th>**</th>\n",
       "      <th>***</th>\n",
       "      <th>*****************</th>\n",
       "      <th>*if</th>\n",
       "      <th>...</th>\n",
       "      <th>—mr</th>\n",
       "      <th>—our</th>\n",
       "      <th>——————————————</th>\n",
       "      <th>†</th>\n",
       "      <th>††</th>\n",
       "      <th>•</th>\n",
       "      <th>●</th>\n",
       "      <th>◻</th>\n",
       "      <th>☐</th>\n",
       "      <th>☒</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>0.012492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058139</td>\n",
       "      <td>0.008176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9058 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          \u0001        ''  'at   'execut      'for  'reason   **  ***  \\\n",
       "0  0.000000  0.052022  0.0  0.000000  0.000000      0.0  0.0  0.0   \n",
       "1  0.000475  0.010278  0.0  0.000000  0.001036      0.0  0.0  0.0   \n",
       "2  0.000000  0.110367  0.0  0.000000  0.000000      0.0  0.0  0.0   \n",
       "3  0.000000  0.078447  0.0  0.005178  0.000000      0.0  0.0  0.0   \n",
       "4  0.000000  0.055112  0.0  0.000000  0.000000      0.0  0.0  0.0   \n",
       "\n",
       "   *****************       *if    ...     —mr  —our  ——————————————    †   ††  \\\n",
       "0           0.004747  0.000000    ...     0.0   0.0             0.0  0.0  0.0   \n",
       "1           0.000000  0.000000    ...     0.0   0.0             0.0  0.0  0.0   \n",
       "2           0.000000  0.000000    ...     0.0   0.0             0.0  0.0  0.0   \n",
       "3           0.000000  0.005178    ...     0.0   0.0             0.0  0.0  0.0   \n",
       "4           0.000000  0.000000    ...     0.0   0.0             0.0  0.0  0.0   \n",
       "\n",
       "          •         ●    ◻         ☐         ☒  \n",
       "0  0.124340  0.000000  0.0  0.017055  0.012492  \n",
       "1  0.000000  0.000000  0.0  0.002338  0.001027  \n",
       "2  0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "3  0.000000  0.026151  0.0  0.058139  0.008176  \n",
       "4  0.068175  0.000000  0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 9058 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizor = TfidfVectorizer(input=\"filename\",tokenizer=stem_tokenizer,stop_words=stop_words)\n",
    "tf_idf = tf_idf_vectorizor.fit_transform(documents[\"file_name\"])\n",
    "tf_idf_norm = normalize(tf_idf)\n",
    "tf_idf_array = tf_idf_norm.toarray()\n",
    "pd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T14:01:55.755276Z",
     "start_time": "2017-11-24T14:01:55.747404Z"
    }
   },
   "source": [
    "### Using Digrams as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T14:03:26.802775Z",
     "start_time": "2017-11-24T14:03:26.794901Z"
    }
   },
   "source": [
    "Instead of using *words* counts as features, we can use *pairs of words*.\n",
    "This are called **digrams**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:01:42.046984Z",
     "start_time": "2019-09-19T17:01:42.043733Z"
    }
   },
   "outputs": [],
   "source": [
    "digramVectorizer=CountVectorizer(input=\"filename\",tokenizer=stem_tokenizer,stop_words=stop_words,ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:07.246736Z",
     "start_time": "2019-09-19T17:01:42.048425Z"
    }
   },
   "outputs": [],
   "source": [
    "X_digram=digramVectorizer.fit_transform(documents[\"file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.145558Z",
     "start_time": "2019-09-19T17:02:32.130146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit state 88157\n",
      "state secur 81484\n",
      "secur exchang 77564\n",
      "exchang commiss 37717\n",
      "commiss washington 24117\n"
     ]
    }
   ],
   "source": [
    "digrams=list(digramVectorizer.vocabulary_.keys())[:5] # only get the first 5 digrams\n",
    "for digram in digrams:\n",
    "    print( digram,digramVectorizer.vocabulary_[digram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.157870Z",
     "start_time": "2019-09-19T17:02:32.147649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93426"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_digram=X_digram.shape[1]\n",
    "V_digram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Trained models for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a text model (calling `fit_transform`) is slow because we need to process all the documents in the training corpus.\n",
    "\n",
    "Sometimes it is convenient to save pre-trained models to disk for reuse later. We can also save the pre-processed test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='preprocessing'\n",
    "set_vectorizer_filename=data_dir+\"/set_vectorizer.p\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.284626Z",
     "start_time": "2019-09-19T17:02:32.179011Z"
    }
   },
   "outputs": [],
   "source": [
    "set_vectorizer_filename=   data_dir+\"/set_vectorizer.p\"\n",
    "set_features_filename=     data_dir+\"/set_features.p\"\n",
    "pickle.dump(setVectorizer, open( set_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_set,         open( set_features_filename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.376893Z",
     "start_time": "2019-09-19T17:02:32.286050Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer_filename=   data_dir+\"/count_vectorizer.p\"\n",
    "count_features_filename=     data_dir+\"/count_features.p\"\n",
    "pickle.dump(countVectorizer, open( count_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X,             open( count_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.521318Z",
     "start_time": "2019-09-19T17:02:32.378539Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_filename=   data_dir+\"/tfidf_vectorizer.p\"\n",
    "tfidf_features_filename=     data_dir+\"/tfidf_features.p\"\n",
    "pickle.dump(tfidfVectorizer, open( tfidf_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(Xi,              open( tfidf_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:33.807752Z",
     "start_time": "2019-09-19T17:02:32.522927Z"
    }
   },
   "outputs": [],
   "source": [
    "digram_vectorizer_filename=   data_dir+\"/digram_vectorizer.p\"\n",
    "digram_features_filename=     data_dir+\"/digram_features.p\"\n",
    "pickle.dump(digramVectorizer, open( digram_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_digram,              open( digram_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPLEMENT LATER WHEN WE HAVE A TRAIN/TEST SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping new documents  into an already learned vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing a test classifier on new documents we need to keep the vocabulary **fixed**.\n",
    "\n",
    "We call `transform`, not `fit_transform` on the `sklearn` vectorizer.\n",
    "\n",
    "Words in new documents, will get mapped to the columns learned on the training set.\n",
    "New words not in the original vocabulary will be ignored.\n",
    "\n",
    "If we called `fit_transform` instead the indexing of words would be all **scrambled up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:00.630802Z",
     "start_time": "2019-09-19T16:59:37.198317Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test=countVectorizer.transform(test_documents[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the word counts of a few new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:00.640005Z",
     "start_time": "2019-09-19T17:00:00.631886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"doc\",\"word\"+\" \"*11,\"dim\",\"count\",sep=\"\\t\")\n",
    "for i1 in range(2):\n",
    "    for word in words:\n",
    "        document=X_test[i1]\n",
    "        dimension=countVectorizer.vocabulary_[word]\n",
    "        print(i1,f\"{word:15}\",dimension,document[0,dimension],sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "497px",
    "left": "0px",
    "right": "auto",
    "top": "107px",
    "width": "314px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
