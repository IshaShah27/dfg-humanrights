{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook illustrates the translation of complex objects (plain text documents) into a set of features suitable for training a machine learning algorithm.\n",
    "\n",
    "We will use the simplest of such models the **Bag of Words** on the [Reuters 50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50) collection of texts.\n",
    "\n",
    "**Note** If the original link to the source data above is down. You can get a google drive copy of the C50 Reuters dataset from [here](https://drive.google.com/drive/folders/1kKFrHulkbxknPDEp9A1f1KwIYzyfcDME?usp=sharing)\n",
    "\n",
    "In this notebook we will:\n",
    "1. Discuss a pipeline to transform files into list of words.\n",
    "2. Implement a few document similarity metrics.\n",
    "3. Compare `sklearn` document vectorizer implementations to our own.\n",
    "4.  Save some preprocessed data sets for later reuse.\n",
    "\n",
    "In practice we will want to use `sklearn` vectorizers, but here we re-implement them ourselves so that we understand all the subtle details of their definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:58:43.068585Z",
     "start_time": "2019-09-19T16:58:42.339933Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Domain specific libraries to handle text\n",
    "#\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Fist time Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook downloads some data, and generates some pre-processed files we will neede later.\n",
    "\n",
    "Set the `fist_time` flag below to True **once**, after that, it will be faster to run the notebook\n",
    "with the flag set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T16:58:43.072576Z",
     "start_time": "2019-09-19T16:58:43.070053Z"
    }
   },
   "outputs": [],
   "source": [
    "first_time=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time: \n",
    "    documents=pd.read_csv(\"/Users/Emily/Desktop/dfg-humanrights/filenames.csv\",index_col=\"document_id\")\n",
    "    documents['label']=documents['file_name'].apply(lambda x: x.split('_')[0])\n",
    "    documents['file_name']=documents['file_name'].apply(lambda x: \"clean_text3/\"+ x )\n",
    "    documents.to_csv('filenames.csv')\n",
    "else: \n",
    "    documents=pd.read_csv(\"/Users/Emily/Desktop/dfg-humanrights/filenames.csv\",index_col=\"document_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag Of Words Document Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest models of a document is the **Bag of Words** model:\n",
    "\n",
    "we **ignore word ordering** and represent a\n",
    "document by the list of words that it containts.\n",
    "\n",
    "We still have a lot of choices we could make:\n",
    "\n",
    "* Does punctuation like `.` and `?` count as words or do we ignore them?\n",
    "* is `New York` one word or two, what about `U.S.`?\n",
    "* Do `car` and `cars` count as different words?\n",
    "* What about `safe` and `safely`\n",
    "* What do we do about miss-spelled words, do we try to fix them?\n",
    "* Do we consider different capitalizations of the same word:  `Car` versus `car`?\n",
    "* Do we include high frequency, low information content words such as `a` and `the` in our bag of words?\n",
    "\n",
    "All this choices are **problem dependent**. If we have a particular ML problem to solve we will use our **domain** knowledge to resolve those questions in the context of that specific task.\n",
    "\n",
    "Today we just illustrate how to put together a **data pre-processing** pipeline. \n",
    "\n",
    "The pipeline is designed in such a way that it will be easier later to change our answer to any of those questions and control how we prepare the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use a default pipeline that is both sensible and simple to implement:\n",
    "* We remove punctuation\n",
    "* `New York` counts as two words but  `U.S` counts as one.\n",
    "* `car` and \"`cars` are the same word, same for `safe`, and `safely`.\n",
    "* miss-spelled words count as different words.\n",
    "* We remove capitalization so `Car` and `car` count as the same word.\n",
    "* We remove high frequency such as `a` and `the`.\n",
    "\n",
    "We will rely on python's **NLTK** (Natural Language Toolkit) library to perform tasks that require **domain** knowledge about\n",
    "text processing:\n",
    "* **tokenization**: breaking character streams into words\n",
    "* **stemming**: normalizing words into their roots: `cars` -> `car`, `safely` -> `safe`\n",
    "* **stop word removal**: high frequency, low information words that we will consider just *noise* and ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function that goes from text to stems for reuse later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to run the stop words through stemmer (so that they will we identified and removed from text). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Consider Adding: \n",
    "\n",
    "-expanding the stop words list\n",
    "\n",
    "-removing specific words that appear more than 80% (arbitrary threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from html import unescape\n",
    "\n",
    "# create a spaCy tokenizer\n",
    "#spacy.load('en')\n",
    "#lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def stem_tokenizer(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return [porter_stemmer.stem(token) for token in word_tokenize(text.lower())]\n",
    "\n",
    "# remove html entities from docs and\n",
    "# set everything to lowercase\n",
    "def my_preprocessor(text):\n",
    "    if (type(text) is not list):\n",
    "        text=text.split('\\n')\n",
    "    text=\" \".join(text).replace(\"\\n\",\" \").replace('\\u200b', '').replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "# tokenize the doc and lemmatize its tokens\n",
    "def my_tokenizer(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    punctuation=list(string.punctuation)\n",
    "    stop0=\" \".join(stopwords.words(\"english\")+punctuation)\n",
    "    stop_words=set(stem_tokenizer(stop0))\n",
    "    stem_list=stem_tokenizer(text)\n",
    "    used_list=[token for token in stem_list if token not in stop_words]\n",
    "    return used_list\n",
    "\n",
    "# create a dataframe from a word matrix\n",
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: run preprocessor and tokenizer method on document sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unit',\n",
       " 'state',\n",
       " 'secur',\n",
       " 'exchang',\n",
       " 'commiss',\n",
       " 'washington',\n",
       " 'dc',\n",
       " '20549',\n",
       " 'schedul',\n",
       " '14a',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'pursuant',\n",
       " 'section',\n",
       " '14',\n",
       " 'secur',\n",
       " 'exchang',\n",
       " 'act',\n",
       " '1934',\n",
       " 'amend',\n",
       " 'file',\n",
       " 'registr',\n",
       " 'ý',\n",
       " 'file',\n",
       " 'parti',\n",
       " 'registr',\n",
       " '¨',\n",
       " 'check',\n",
       " 'appropri',\n",
       " 'box',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " '¨',\n",
       " 'preliminari',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " '¨',\n",
       " 'confidenti',\n",
       " 'use',\n",
       " 'commiss',\n",
       " 'permit',\n",
       " 'rule',\n",
       " '14a-6',\n",
       " 'e',\n",
       " '2',\n",
       " 'ý',\n",
       " 'definit',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " '¨',\n",
       " 'definit',\n",
       " 'addit',\n",
       " 'materi',\n",
       " '¨',\n",
       " 'solicit',\n",
       " 'materi',\n",
       " 'rule',\n",
       " '14a-12',\n",
       " 'end',\n",
       " 'tabl',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'name',\n",
       " 'registr',\n",
       " 'specifi',\n",
       " 'charter',\n",
       " 'name',\n",
       " 'person',\n",
       " 'file',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'registr',\n",
       " 'payment',\n",
       " 'file',\n",
       " 'fee',\n",
       " 'check',\n",
       " 'appropri',\n",
       " 'box',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " 'ý',\n",
       " 'fee',\n",
       " 'requir',\n",
       " '¨',\n",
       " 'fee',\n",
       " 'comput',\n",
       " 'tabl',\n",
       " 'per',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '14a-6',\n",
       " '1',\n",
       " '0-11',\n",
       " '1',\n",
       " 'titl',\n",
       " 'class',\n",
       " 'secur',\n",
       " 'transact',\n",
       " 'appli',\n",
       " '2',\n",
       " 'aggreg',\n",
       " 'number',\n",
       " 'secur',\n",
       " 'transact',\n",
       " 'appli',\n",
       " '3',\n",
       " 'per',\n",
       " 'unit',\n",
       " 'price',\n",
       " 'underli',\n",
       " 'valu',\n",
       " 'transact',\n",
       " 'comput',\n",
       " 'pursuant',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'amount',\n",
       " 'file',\n",
       " 'fee',\n",
       " 'calcul',\n",
       " 'state',\n",
       " 'determin',\n",
       " '4',\n",
       " 'propos',\n",
       " 'maximum',\n",
       " 'aggreg',\n",
       " 'valu',\n",
       " 'transact',\n",
       " '5',\n",
       " 'total',\n",
       " 'fee',\n",
       " 'paid',\n",
       " '¨',\n",
       " 'fee',\n",
       " 'paid',\n",
       " 'previous',\n",
       " 'preliminari',\n",
       " 'materi',\n",
       " '¨',\n",
       " 'check',\n",
       " 'box',\n",
       " 'part',\n",
       " 'fee',\n",
       " 'offset',\n",
       " 'provid',\n",
       " 'exchang',\n",
       " 'act',\n",
       " 'rule',\n",
       " '0-11',\n",
       " '2',\n",
       " 'identifi',\n",
       " 'file',\n",
       " 'offset',\n",
       " 'fee',\n",
       " 'paid',\n",
       " 'previous',\n",
       " 'identifi',\n",
       " 'previou',\n",
       " 'file',\n",
       " 'registr',\n",
       " 'statement',\n",
       " 'number',\n",
       " 'form',\n",
       " 'schedul',\n",
       " 'date',\n",
       " 'file',\n",
       " '1',\n",
       " 'amount',\n",
       " 'previous',\n",
       " 'paid',\n",
       " '2',\n",
       " 'form',\n",
       " 'schedul',\n",
       " 'registr',\n",
       " 'statement',\n",
       " '3',\n",
       " 'file',\n",
       " 'parti',\n",
       " '4',\n",
       " 'date',\n",
       " 'file',\n",
       " 'end',\n",
       " 'tabl',\n",
       " '400',\n",
       " 'galleria',\n",
       " 'parkway',\n",
       " 's.e.',\n",
       " 'suit',\n",
       " '300',\n",
       " 'atlanta',\n",
       " 'georgia',\n",
       " '30339',\n",
       " 'march',\n",
       " '28',\n",
       " '2019',\n",
       " 'fellow',\n",
       " 'sharehold',\n",
       " 'pleasur',\n",
       " 'invit',\n",
       " 'attend',\n",
       " '2019',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'held',\n",
       " 'wednesday',\n",
       " 'may',\n",
       " '8',\n",
       " '2019',\n",
       " '9:00',\n",
       " 'a.m.',\n",
       " 'local',\n",
       " 'time',\n",
       " 'georgian',\n",
       " 'club',\n",
       " 'locat',\n",
       " '100',\n",
       " 'galleria',\n",
       " 'parkway',\n",
       " 'se',\n",
       " '17th',\n",
       " 'floor',\n",
       " 'atlanta',\n",
       " 'georgia',\n",
       " '30339.',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'begin',\n",
       " 'discuss',\n",
       " 'vote',\n",
       " 'matter',\n",
       " 'describ',\n",
       " 'accompani',\n",
       " 'notic',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'follow',\n",
       " 'report',\n",
       " 'aaron',\n",
       " 'financi',\n",
       " 'perform',\n",
       " 'oper',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'critic',\n",
       " 'corpor',\n",
       " 'govern',\n",
       " 'process',\n",
       " 'use',\n",
       " 'document',\n",
       " 'discuss',\n",
       " 'propos',\n",
       " 'submit',\n",
       " 'vote',\n",
       " 'sharehold',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'solicit',\n",
       " 'vote',\n",
       " 'propos',\n",
       " 'provid',\n",
       " 'inform',\n",
       " 'board',\n",
       " 'director',\n",
       " 'execut',\n",
       " 'offic',\n",
       " 'inform',\n",
       " 'step',\n",
       " 'take',\n",
       " 'fulfil',\n",
       " 'respons',\n",
       " 'sharehold',\n",
       " 'vote',\n",
       " 'import',\n",
       " 'us',\n",
       " 'broker',\n",
       " 'vote',\n",
       " 'certain',\n",
       " 'propos',\n",
       " 'without',\n",
       " 'instruct',\n",
       " 'pleas',\n",
       " 'use',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'voter',\n",
       " 'instruct',\n",
       " 'form',\n",
       " 'inform',\n",
       " 'us',\n",
       " 'broker',\n",
       " 'like',\n",
       " 'vote',\n",
       " 'share',\n",
       " 'propos',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'instruct',\n",
       " 'vote',\n",
       " 'pleas',\n",
       " 'refer',\n",
       " 'notic',\n",
       " 'receiv',\n",
       " 'mail',\n",
       " 'request',\n",
       " 'hard',\n",
       " 'copi',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'enclos',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'see',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'behalf',\n",
       " 'manag',\n",
       " 'director',\n",
       " 'want',\n",
       " 'thank',\n",
       " 'continu',\n",
       " 'support',\n",
       " 'confid',\n",
       " 'aaron',\n",
       " 'sincer',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " 'ray',\n",
       " 'm.',\n",
       " 'robinson',\n",
       " 'john',\n",
       " 'w.',\n",
       " 'robinson',\n",
       " 'iii',\n",
       " 'chairman',\n",
       " 'board',\n",
       " 'presid',\n",
       " 'chief',\n",
       " 'execut',\n",
       " 'offic',\n",
       " 'end',\n",
       " 'tabl',\n",
       " '400',\n",
       " 'galleria',\n",
       " 'parkway',\n",
       " 's.e.',\n",
       " 'suit',\n",
       " '300',\n",
       " 'atlanta',\n",
       " 'georgia',\n",
       " '30339',\n",
       " 'notic',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " 'held',\n",
       " 'may',\n",
       " '8',\n",
       " '2019',\n",
       " '2019',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'refer',\n",
       " 'aaron',\n",
       " \"''\",\n",
       " 'compani',\n",
       " \"''\",\n",
       " 'held',\n",
       " 'wednesday',\n",
       " 'may',\n",
       " '8',\n",
       " '2019',\n",
       " '9:00',\n",
       " 'a.m.',\n",
       " 'local',\n",
       " 'time',\n",
       " 'georgian',\n",
       " 'club',\n",
       " 'locat',\n",
       " '100',\n",
       " 'galleria',\n",
       " 'parkway',\n",
       " 'se',\n",
       " '17th',\n",
       " 'floor',\n",
       " 'atlanta',\n",
       " 'georgia',\n",
       " '30339',\n",
       " 'purpos',\n",
       " 'consid',\n",
       " 'vote',\n",
       " 'follow',\n",
       " 'item',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " '1.',\n",
       " 'elect',\n",
       " 'eight',\n",
       " 'director',\n",
       " 'serv',\n",
       " 'term',\n",
       " 'expir',\n",
       " '2020',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " '2.',\n",
       " 'vote',\n",
       " 'non-bind',\n",
       " 'advisori',\n",
       " 'resolut',\n",
       " 'approv',\n",
       " 'aaron',\n",
       " 'execut',\n",
       " 'compens',\n",
       " '3.',\n",
       " 'adopt',\n",
       " 'approv',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'amend',\n",
       " 'restat',\n",
       " '2015',\n",
       " 'equiti',\n",
       " 'incent',\n",
       " 'plan',\n",
       " '4.',\n",
       " 'ratifi',\n",
       " 'appoint',\n",
       " 'ernst',\n",
       " 'young',\n",
       " 'llp',\n",
       " 'aaron',\n",
       " 'independ',\n",
       " 'regist',\n",
       " 'public',\n",
       " 'account',\n",
       " 'firm',\n",
       " '2019',\n",
       " '5.',\n",
       " 'transact',\n",
       " 'busi',\n",
       " 'may',\n",
       " 'properli',\n",
       " 'come',\n",
       " 'meet',\n",
       " 'adjourn',\n",
       " 'postpon',\n",
       " 'thereof',\n",
       " 'end',\n",
       " 'tabl',\n",
       " 'inform',\n",
       " 'relat',\n",
       " 'item',\n",
       " 'provid',\n",
       " 'accompani',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'sharehold',\n",
       " 'record',\n",
       " 'shown',\n",
       " 'stock',\n",
       " 'transfer',\n",
       " 'book',\n",
       " 'aaron',\n",
       " 'march',\n",
       " '4',\n",
       " '2019',\n",
       " 'entitl',\n",
       " 'notic',\n",
       " 'vote',\n",
       " 'meet',\n",
       " 'hold',\n",
       " 'share',\n",
       " 'bank',\n",
       " 'broker',\n",
       " 'nomine',\n",
       " 'commonli',\n",
       " 'known',\n",
       " 'hold',\n",
       " 'share',\n",
       " 'street',\n",
       " 'name',\n",
       " \"''\",\n",
       " 'contact',\n",
       " 'firm',\n",
       " 'hold',\n",
       " 'share',\n",
       " 'instruct',\n",
       " 'vote',\n",
       " 'share',\n",
       " 'sharehold',\n",
       " 'record',\n",
       " 'march',\n",
       " '4',\n",
       " '2019',\n",
       " 'strongli',\n",
       " 'encourag',\n",
       " 'vote',\n",
       " 'one',\n",
       " 'follow',\n",
       " 'way',\n",
       " 'whether',\n",
       " 'plan',\n",
       " 'attend',\n",
       " 'annual',\n",
       " 'meet',\n",
       " '1',\n",
       " 'telephon',\n",
       " '2',\n",
       " 'via',\n",
       " 'internet',\n",
       " '3',\n",
       " 'complet',\n",
       " 'sign',\n",
       " 'date',\n",
       " 'written',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'return',\n",
       " 'promptli',\n",
       " 'address',\n",
       " 'indic',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " 'order',\n",
       " 'board',\n",
       " 'director',\n",
       " 'robert',\n",
       " 'w.',\n",
       " 'kamerschen',\n",
       " 'execut',\n",
       " 'vice',\n",
       " 'presid',\n",
       " 'gener',\n",
       " 'counsel',\n",
       " 'chief',\n",
       " 'administr',\n",
       " 'offic',\n",
       " 'corpor',\n",
       " 'secretari',\n",
       " 'atlanta',\n",
       " 'georgia',\n",
       " 'march',\n",
       " '28',\n",
       " '2019',\n",
       " 'end',\n",
       " 'tabl',\n",
       " 'import',\n",
       " 'notic',\n",
       " 'regard',\n",
       " 'avail',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'held',\n",
       " 'may',\n",
       " '8',\n",
       " '2019.',\n",
       " 'pleas',\n",
       " 'announc',\n",
       " 'deliv',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " '2019',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " 'via',\n",
       " 'internet',\n",
       " 'deliv',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'via',\n",
       " 'internet',\n",
       " 'secur',\n",
       " 'exchang',\n",
       " 'commiss',\n",
       " 'requir',\n",
       " 'us',\n",
       " 'mail',\n",
       " 'notic',\n",
       " 'sharehold',\n",
       " 'notifi',\n",
       " 'materi',\n",
       " 'avail',\n",
       " 'internet',\n",
       " 'materi',\n",
       " 'may',\n",
       " 'access',\n",
       " 'notic',\n",
       " 'refer',\n",
       " 'notic',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " \"''\",\n",
       " 'mail',\n",
       " 'sharehold',\n",
       " 'march',\n",
       " '28',\n",
       " '2019.',\n",
       " 'notic',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'instruct',\n",
       " 'may',\n",
       " 'vote',\n",
       " 'proxi',\n",
       " 'via',\n",
       " 'internet',\n",
       " 'telephon',\n",
       " 'request',\n",
       " 'full',\n",
       " 'set',\n",
       " 'print',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'includ',\n",
       " 'proxi',\n",
       " 'card',\n",
       " 'return',\n",
       " 'mail',\n",
       " 'like',\n",
       " 'receiv',\n",
       " 'print',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'follow',\n",
       " 'instruct',\n",
       " 'contain',\n",
       " 'notic',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'unless',\n",
       " 'request',\n",
       " 'receiv',\n",
       " 'print',\n",
       " 'proxi',\n",
       " 'materi',\n",
       " 'mail',\n",
       " 'proxi',\n",
       " 'statement',\n",
       " 'annual',\n",
       " 'report',\n",
       " 'avail',\n",
       " 'free',\n",
       " 'charg',\n",
       " 'websit',\n",
       " 'http',\n",
       " '//www.aarons.com/proxi',\n",
       " 'http',\n",
       " '//www.aarons.com/annualreport',\n",
       " 'respect',\n",
       " 'http',\n",
       " '//www.envisionreports.com/aan',\n",
       " 'begin',\n",
       " 'tabl',\n",
       " 'proxi',\n",
       " 'summari',\n",
       " '1',\n",
       " 'matter',\n",
       " 'vote',\n",
       " '4',\n",
       " 'propos',\n",
       " '1',\n",
       " 'elect',\n",
       " 'eight',\n",
       " 'director',\n",
       " '4',\n",
       " 'propos',\n",
       " '2',\n",
       " 'advisori',\n",
       " 'vote',\n",
       " 'approv',\n",
       " 'execut',\n",
       " 'offic',\n",
       " 'compens',\n",
       " '5',\n",
       " 'propos',\n",
       " '3',\n",
       " 'approv',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'amend',\n",
       " 'restat',\n",
       " '2015',\n",
       " 'equiti',\n",
       " 'incent',\n",
       " 'plan',\n",
       " '6',\n",
       " 'propos',\n",
       " '4',\n",
       " 'ratif',\n",
       " 'appoint',\n",
       " 'ernst',\n",
       " 'young',\n",
       " 'llp',\n",
       " 'independ',\n",
       " 'regist',\n",
       " 'public',\n",
       " 'account',\n",
       " 'firm',\n",
       " '14',\n",
       " 'govern',\n",
       " '15',\n",
       " 'nomine',\n",
       " 'serv',\n",
       " 'director',\n",
       " '15',\n",
       " 'execut',\n",
       " 'offic',\n",
       " 'director',\n",
       " '17',\n",
       " 'composit',\n",
       " 'meet',\n",
       " 'committe',\n",
       " 'board',\n",
       " 'director',\n",
       " '18',\n",
       " 'assess',\n",
       " 'director',\n",
       " 'candid',\n",
       " 'requir',\n",
       " 'qualif',\n",
       " '19',\n",
       " 'sharehold',\n",
       " 'recommend',\n",
       " 'nomin',\n",
       " 'elect',\n",
       " 'board',\n",
       " '20',\n",
       " 'board',\n",
       " 'leadership',\n",
       " 'structur',\n",
       " '20',\n",
       " 'board',\n",
       " 'director',\n",
       " 'committe',\n",
       " 'evalu',\n",
       " '21',\n",
       " 'board',\n",
       " 'role',\n",
       " 'risk',\n",
       " 'oversight',\n",
       " '21',\n",
       " 'social',\n",
       " 'environment',\n",
       " 'respons',\n",
       " '21',\n",
       " 'board',\n",
       " 'workplac',\n",
       " 'divers',\n",
       " '22',\n",
       " 'compens',\n",
       " 'committe',\n",
       " 'interlock',\n",
       " 'insid',\n",
       " 'particip',\n",
       " '22',\n",
       " 'section',\n",
       " '16',\n",
       " 'benefici',\n",
       " 'ownership',\n",
       " 'report',\n",
       " 'complianc',\n",
       " '22',\n",
       " 'non-manag',\n",
       " 'director',\n",
       " 'compens',\n",
       " '2018',\n",
       " '22',\n",
       " 'stock',\n",
       " 'ownership',\n",
       " 'guidelin',\n",
       " '23',\n",
       " 'compens',\n",
       " 'discuss',\n",
       " 'analysi',\n",
       " '24',\n",
       " 'execut',\n",
       " 'summari',\n",
       " '24',\n",
       " 'object',\n",
       " 'execut',\n",
       " 'compens',\n",
       " '25',\n",
       " 'compens',\n",
       " 'process',\n",
       " 'summari',\n",
       " '2018',\n",
       " '26',\n",
       " 'benchmark',\n",
       " '26',\n",
       " 'compon',\n",
       " 'execut',\n",
       " 'compens',\n",
       " 'program',\n",
       " '27',\n",
       " 'base',\n",
       " 'salari',\n",
       " '28',\n",
       " 'annual',\n",
       " 'cash',\n",
       " 'incent',\n",
       " 'award',\n",
       " '29',\n",
       " 'long-term',\n",
       " 'equiti',\n",
       " 'incent',\n",
       " 'award',\n",
       " '31',\n",
       " 'execut',\n",
       " 'compens',\n",
       " 'polici',\n",
       " '34',\n",
       " 'execut',\n",
       " 'benefit',\n",
       " 'perquisit',\n",
       " '34',\n",
       " 'employ',\n",
       " 'agreement',\n",
       " 'post',\n",
       " 'termin',\n",
       " 'protect',\n",
       " '35',\n",
       " 'polici',\n",
       " 'compens',\n",
       " 'tax',\n",
       " 'deduct',\n",
       " '35',\n",
       " 'compens',\n",
       " 'committe',\n",
       " 'report',\n",
       " '36',\n",
       " 'execut',\n",
       " 'compens',\n",
       " '37',\n",
       " 'summari',\n",
       " 'compens',\n",
       " 'tabl',\n",
       " '37',\n",
       " 'grant',\n",
       " 'plan-bas',\n",
       " 'award',\n",
       " '2018',\n",
       " '38',\n",
       " 'employ',\n",
       " 'agreement',\n",
       " 'name',\n",
       " 'execut',\n",
       " 'offic',\n",
       " '38',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " '2015',\n",
       " 'equiti',\n",
       " 'incent',\n",
       " 'plan',\n",
       " '39',\n",
       " 'amend',\n",
       " 'restat',\n",
       " '2001',\n",
       " 'stock',\n",
       " 'option',\n",
       " 'incent',\n",
       " 'award',\n",
       " 'plan',\n",
       " '40',\n",
       " 'aaron',\n",
       " 'inc.',\n",
       " 'employe',\n",
       " 'stock',\n",
       " 'purchas',\n",
       " 'plan',\n",
       " '43',\n",
       " 'outstand',\n",
       " 'equiti',\n",
       " 'award',\n",
       " '2018',\n",
       " 'fiscal',\n",
       " 'year-end',\n",
       " '44',\n",
       " 'option',\n",
       " 'exercis',\n",
       " 'stock',\n",
       " 'vest',\n",
       " '2018',\n",
       " '43',\n",
       " 'pension',\n",
       " 'benefit',\n",
       " '43',\n",
       " 'nonqualifi',\n",
       " 'defer',\n",
       " 'compens',\n",
       " 'decemb',\n",
       " '31',\n",
       " '2018',\n",
       " '43',\n",
       " 'potenti',\n",
       " 'payment',\n",
       " 'upon',\n",
       " 'termin',\n",
       " 'chang',\n",
       " 'control',\n",
       " '44',\n",
       " 'secur',\n",
       " 'author',\n",
       " 'issuanc',\n",
       " 'equiti',\n",
       " 'compens',\n",
       " 'plan',\n",
       " '48',\n",
       " 'ceo',\n",
       " 'pay',\n",
       " 'ratio',\n",
       " 'disclosur',\n",
       " '51',\n",
       " 'audit',\n",
       " 'committe',\n",
       " 'report',\n",
       " '50',\n",
       " 'audit',\n",
       " 'matter',\n",
       " '53',\n",
       " 'fee',\n",
       " 'bill',\n",
       " 'last',\n",
       " 'two',\n",
       " 'fiscal',\n",
       " 'year',\n",
       " '53',\n",
       " 'approv',\n",
       " 'auditor',\n",
       " 'servic',\n",
       " '55',\n",
       " 'benefici',\n",
       " 'ownership',\n",
       " 'common',\n",
       " 'stock',\n",
       " '56',\n",
       " 'certain',\n",
       " 'relationship',\n",
       " 'relat',\n",
       " 'transact',\n",
       " '58',\n",
       " 'polici',\n",
       " 'procedur',\n",
       " 'deal',\n",
       " 'review',\n",
       " 'approv',\n",
       " 'ratif',\n",
       " 'relat',\n",
       " 'parti',\n",
       " 'transact',\n",
       " '58',\n",
       " 'relat',\n",
       " 'parti',\n",
       " 'transact',\n",
       " '58',\n",
       " 'question',\n",
       " 'answer',\n",
       " 'vote',\n",
       " 'annual',\n",
       " 'meet',\n",
       " '59',\n",
       " 'addit',\n",
       " 'inform',\n",
       " '63',\n",
       " 'sharehold',\n",
       " 'propos',\n",
       " '2020',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'sharehold',\n",
       " '63',\n",
       " 'household',\n",
       " 'annual',\n",
       " 'meet',\n",
       " 'materi',\n",
       " '64',\n",
       " 'commun',\n",
       " 'board',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(documents[\"file_name\"][0], 'rb') as fh:\n",
    "    doc = fh.read().decode(\"utf-8\")\n",
    "text=my_preprocessor(doc)\n",
    "my_tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to Entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0001</th>\n",
       "      <th>''</th>\n",
       "      <th>'74</th>\n",
       "      <th>**</th>\n",
       "      <th>+/-</th>\n",
       "      <th>+1</th>\n",
       "      <th>--</th>\n",
       "      <th>-1,667</th>\n",
       "      <th>-145.0</th>\n",
       "      <th>-2,500</th>\n",
       "      <th>...</th>\n",
       "      <th>—investors—corpor</th>\n",
       "      <th>—potenti</th>\n",
       "      <th>—who</th>\n",
       "      <th>•</th>\n",
       "      <th>∎</th>\n",
       "      <th>▪</th>\n",
       "      <th>☐</th>\n",
       "      <th>☒</th>\n",
       "      <th>✓</th>\n",
       "      <th>✔</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.110153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019521</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085005</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.045643</td>\n",
       "      <td>0.085691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07868</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.345878</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             \u0001        ''       '74        **       +/-        +1        --  \\\n",
       "Doc0  0.032877  0.110153  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.049414  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.123771  0.000000  0.003591  0.000000  0.001197  0.017955   \n",
       "Doc3  0.000000  0.085005  0.002337  0.000000  0.003116  0.000000  0.000000   \n",
       "Doc4  0.000000  0.053197  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        -1,667    -145.0    -2,500    ...     —investors—corpor  —potenti  \\\n",
       "Doc0  0.000000  0.000000  0.000000    ...              0.000000  0.002055   \n",
       "Doc1  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "Doc2  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "Doc3  0.001558  0.000779  0.001558    ...              0.000779  0.000000   \n",
       "Doc4  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "\n",
       "          —who         •         ∎         ▪        ☐         ☒         ✓  \\\n",
       "Doc0  0.000000  0.063092  0.000000  0.019521  0.00000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.045516  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.033044  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "Doc3  0.000779  0.045643  0.085691  0.000000  0.07868  0.002337  0.345878   \n",
       "Doc4  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "             ✔  \n",
       "Doc0  0.000000  \n",
       "Doc1  0.016881  \n",
       "Doc2  0.000000  \n",
       "Doc3  0.000000  \n",
       "Doc4  0.000000  \n",
       "\n",
       "[5 rows x 6532 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(input='filename', preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n",
    "X_tfidf=tfidf_vectorizer.fit_transform(documents[\"file_name\"].head(5))\n",
    "X_tfidf = normalize(X_tfidf) #explains why we normalize: https://www.quora.com/What-is-the-benefit-of-normalization-in-the-tf-idf-algorithm\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names()\n",
    "# create a dataframe from the matrix\n",
    "wm2df(X_tfidf, tdidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0001</th>\n",
       "      <th>''</th>\n",
       "      <th>'74</th>\n",
       "      <th>**</th>\n",
       "      <th>+/-</th>\n",
       "      <th>+1</th>\n",
       "      <th>--</th>\n",
       "      <th>-1,667</th>\n",
       "      <th>-145.0</th>\n",
       "      <th>-2,500</th>\n",
       "      <th>...</th>\n",
       "      <th>—investors—corpor</th>\n",
       "      <th>—potenti</th>\n",
       "      <th>—who</th>\n",
       "      <th>•</th>\n",
       "      <th>∎</th>\n",
       "      <th>▪</th>\n",
       "      <th>☐</th>\n",
       "      <th>☒</th>\n",
       "      <th>✓</th>\n",
       "      <th>✔</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.016158</td>\n",
       "      <td>0.113610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098051</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>0.047099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.190107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             \u0001        ''       '74        **       +/-        +1        --  \\\n",
       "Doc0  0.016158  0.113610  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.065449  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.128449  0.000000  0.001776  0.000000  0.000592  0.008879   \n",
       "Doc3  0.000000  0.098051  0.001285  0.000000  0.001713  0.000000  0.000000   \n",
       "Doc4  0.000000  0.065158  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        -1,667    -145.0    -2,500    ...     —investors—corpor  —potenti  \\\n",
       "Doc0  0.000000  0.000000  0.000000    ...              0.000000   0.00101   \n",
       "Doc1  0.000000  0.000000  0.000000    ...              0.000000   0.00000   \n",
       "Doc2  0.000000  0.000000  0.000000    ...              0.000000   0.00000   \n",
       "Doc3  0.000856  0.000428  0.000856    ...              0.000428   0.00000   \n",
       "Doc4  0.000000  0.000000  0.000000    ...              0.000000   0.00000   \n",
       "\n",
       "          —who         •         ∎         ▪         ☐         ☒         ✓  \\\n",
       "Doc0  0.000000  0.055038  0.000000  0.009594  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.050989  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.029005  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc3  0.000428  0.044529  0.047099  0.000000  0.043245  0.001285  0.190107   \n",
       "Doc4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "             ✔  \n",
       "Doc0  0.000000  \n",
       "Doc1  0.010654  \n",
       "Doc2  0.000000  \n",
       "Doc3  0.000000  \n",
       "Doc4  0.000000  \n",
       "\n",
       "[5 rows x 6532 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer=CountVectorizer(input='filename', preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n",
    "X_count=count_vectorizer.fit_transform(documents[\"file_name\"].head(5))\n",
    "X_count = normalize(X_count) #explains why we normalize: https://www.quora.com/What-is-the-benefit-of-normalization-in-the-tf-idf-algorithm\n",
    "count_features = count_vectorizer.get_feature_names()\n",
    "# create a dataframe from the matrix\n",
    "wm2df(X_count, count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0001</th>\n",
       "      <th>''</th>\n",
       "      <th>'74</th>\n",
       "      <th>**</th>\n",
       "      <th>+/-</th>\n",
       "      <th>+1</th>\n",
       "      <th>--</th>\n",
       "      <th>-1,667</th>\n",
       "      <th>-145.0</th>\n",
       "      <th>-2,500</th>\n",
       "      <th>...</th>\n",
       "      <th>—investors—corpor</th>\n",
       "      <th>—potenti</th>\n",
       "      <th>—who</th>\n",
       "      <th>•</th>\n",
       "      <th>∎</th>\n",
       "      <th>▪</th>\n",
       "      <th>☐</th>\n",
       "      <th>☒</th>\n",
       "      <th>✓</th>\n",
       "      <th>✔</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             \u0001        ''       '74        **       +/-        +1        --  \\\n",
       "Doc0  0.018415  0.018415  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.022863  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.020315  0.000000  0.020315  0.000000  0.020315  0.020315   \n",
       "Doc3  0.000000  0.018458  0.018458  0.000000  0.018458  0.000000  0.000000   \n",
       "Doc4  0.000000  0.021330  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        -1,667    -145.0    -2,500    ...     —investors—corpor  —potenti  \\\n",
       "Doc0  0.000000  0.000000  0.000000    ...              0.000000  0.018415   \n",
       "Doc1  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "Doc2  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "Doc3  0.018458  0.018458  0.018458    ...              0.018458  0.000000   \n",
       "Doc4  0.000000  0.000000  0.000000    ...              0.000000  0.000000   \n",
       "\n",
       "          —who         •         ∎         ▪         ☐         ☒         ✓  \\\n",
       "Doc0  0.000000  0.018415  0.000000  0.018415  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000  0.022863  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.020315  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc3  0.018458  0.018458  0.018458  0.000000  0.018458  0.018458  0.018458   \n",
       "Doc4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "             ✔  \n",
       "Doc0  0.000000  \n",
       "Doc1  0.022863  \n",
       "Doc2  0.000000  \n",
       "Doc3  0.000000  \n",
       "Doc4  0.000000  \n",
       "\n",
       "[5 rows x 6532 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_vectorizer=CountVectorizer(input='filename', binary=True, preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n",
    "X_set=set_vectorizer.fit_transform(documents[\"file_name\"].head(5))\n",
    "X_set = normalize(X_set) #explains why we normalize: https://www.quora.com/What-is-the-benefit-of-normalization-in-the-tf-idf-algorithm\n",
    "set_features = set_vectorizer.get_feature_names()\n",
    "# create a dataframe from the matrix\n",
    "wm2df(X_set, set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T14:01:55.755276Z",
     "start_time": "2017-11-24T14:01:55.747404Z"
    }
   },
   "source": [
    "### Using Digrams as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T14:03:26.802775Z",
     "start_time": "2017-11-24T14:03:26.794901Z"
    }
   },
   "source": [
    "Instead of using *words* counts as features, we can use *pairs of words*.\n",
    "This are called **digrams**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:01:42.046984Z",
     "start_time": "2019-09-19T17:01:42.043733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0001 \u0001</th>\n",
       "      <th>\u0001 1</th>\n",
       "      <th>\u0001 begin</th>\n",
       "      <th>\u0001 compon</th>\n",
       "      <th>\u0001 end</th>\n",
       "      <th>\u0001 stock</th>\n",
       "      <th>\u0001 target</th>\n",
       "      <th>'' ''</th>\n",
       "      <th>'' -actual</th>\n",
       "      <th>'' .1</th>\n",
       "      <th>...</th>\n",
       "      <th>✓ sustain</th>\n",
       "      <th>✓ sustanatm</th>\n",
       "      <th>✓ suzann</th>\n",
       "      <th>✓ timothi</th>\n",
       "      <th>✓ train</th>\n",
       "      <th>✓ ✓</th>\n",
       "      <th>✔ 0</th>\n",
       "      <th>✔ 1</th>\n",
       "      <th>✔ 2</th>\n",
       "      <th>✔ 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.01999</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007709</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.007709</td>\n",
       "      <td>0.007709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022973</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          \u0001 \u0001       \u0001 1   \u0001 begin  \u0001 compon     \u0001 end   \u0001 stock  \u0001 target  \\\n",
       "Doc0  0.01999  0.001538  0.010764  0.001538  0.010764  0.003075  0.001538   \n",
       "Doc1  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc2  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc3  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc4  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         '' ''  '' -actual     '' .1    ...     ✓ sustain  ✓ sustanatm  \\\n",
       "Doc0  0.007688    0.000000  0.000000    ...      0.000000     0.000000   \n",
       "Doc1  0.005782    0.000000  0.000000    ...      0.000000     0.000000   \n",
       "Doc2  0.005328    0.000000  0.001776    ...      0.000000     0.000000   \n",
       "Doc3  0.022973    0.001094  0.000000    ...      0.006564     0.006564   \n",
       "Doc4  0.000000    0.000000  0.000000    ...      0.000000     0.000000   \n",
       "\n",
       "      ✓ suzann  ✓ timothi   ✓ train       ✓ ✓       ✔ 0       ✔ 1       ✔ 2  \\\n",
       "Doc0  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc1  0.000000   0.000000  0.000000  0.000000  0.007709  0.003855  0.007709   \n",
       "Doc2  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc3  0.002188   0.002188  0.004376  0.018598  0.000000  0.000000  0.000000   \n",
       "Doc4  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           ✔ 3  \n",
       "Doc0  0.000000  \n",
       "Doc1  0.007709  \n",
       "Doc2  0.000000  \n",
       "Doc3  0.000000  \n",
       "Doc4  0.000000  \n",
       "\n",
       "[5 rows x 53204 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digram_vectorizer=CountVectorizer(input=\"filename\",preprocessor=my_preprocessor, tokenizer=my_tokenizer,ngram_range=(2,2))\n",
    "X_digram=digram_vectorizer.fit_transform(documents[\"file_name\"].head(5))\n",
    "X_digram = normalize(X_digram) #explains why we normalize: https://www.quora.com/What-is-the-benefit-of-normalization-in-the-tf-idf-algorithm\n",
    "digram_features = digram_vectorizer.get_feature_names()\n",
    "# create a dataframe from the matrix\n",
    "wm2df(X_digram, digram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.145558Z",
     "start_time": "2019-09-19T17:02:32.130146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit state 50204\n",
      "state secur 46507\n",
      "secur exchang 44417\n",
      "exchang commiss 22719\n",
      "commiss washington 14778\n"
     ]
    }
   ],
   "source": [
    "digrams=list(digram_vectorizer.vocabulary_.keys())[:5] # only get the first 5 digrams\n",
    "for digram in digrams:\n",
    "    print( digram,digram_vectorizer.vocabulary_[digram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.157870Z",
     "start_time": "2019-09-19T17:02:32.147649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93426"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_digram=X_digram.shape[1]\n",
    "V_digram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Trained models for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a text model (calling `fit_transform`) is slow because we need to process all the documents in the training corpus.\n",
    "\n",
    "Sometimes it is convenient to save pre-trained models to disk for reuse later. We can also save the pre-processed test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='preprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.284626Z",
     "start_time": "2019-09-19T17:02:32.179011Z"
    }
   },
   "outputs": [],
   "source": [
    "set_vectorizer_filename=   data_dir+\"/set_vectorizer.p\"\n",
    "set_features_filename=     data_dir+\"/set_features.p\"\n",
    "pickle.dump(set_vectorizer, open( set_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_set,         open( set_features_filename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.376893Z",
     "start_time": "2019-09-19T17:02:32.286050Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer_filename=   data_dir+\"/count_vectorizer.p\"\n",
    "count_features_filename=     data_dir+\"/count_features.p\"\n",
    "pickle.dump(count_vectorizer, open( count_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_count,             open( count_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:32.521318Z",
     "start_time": "2019-09-19T17:02:32.378539Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_filename=   data_dir+\"/tfidf_vectorizer.p\"\n",
    "tfidf_features_filename=     data_dir+\"/tfidf_features.p\"\n",
    "pickle.dump(tfidf_vectorizer, open( tfidf_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_tfidf,              open( tfidf_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:02:33.807752Z",
     "start_time": "2019-09-19T17:02:32.522927Z"
    }
   },
   "outputs": [],
   "source": [
    "digram_vectorizer_filename=   data_dir+\"/digram_vectorizer.p\"\n",
    "digram_features_filename=     data_dir+\"/digram_features.p\"\n",
    "pickle.dump(digram_vectorizer, open( digram_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X_digram,              open( digram_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "committe\n",
      " \n",
      "compens\n",
      " \n",
      "tabl\n",
      " \n",
      "compani\n",
      " \n",
      "share\n",
      " \n",
      "director\n",
      " \n",
      "stock\n",
      " \n",
      "Topic 1: \n",
      "award\n",
      " \n",
      "stock\n",
      " \n",
      "share\n",
      " \n",
      "plan\n",
      " \n",
      "shall\n",
      " \n",
      "compani\n",
      " \n",
      "—\n",
      " \n",
      "Topic 2: \n",
      "appl\n",
      " \n",
      "rsu\n",
      " \n",
      "sharehold\n",
      " \n",
      "vest\n",
      " \n",
      "grant\n",
      " \n",
      "award\n",
      " \n",
      "cook\n",
      " \n",
      "Topic 3: \n",
      "✓\n",
      " \n",
      "alcoa\n",
      " \n",
      "\n",
      " \n",
      "committe\n",
      " \n",
      "0\n",
      " \n",
      "∎\n",
      " \n",
      "☐\n",
      " \n",
      "Topic 4: \n",
      "aaron\n",
      " \n",
      "robinson\n",
      " \n",
      "director\n",
      " \n",
      "share\n",
      " \n",
      "perform\n",
      " \n",
      "common\n",
      " \n",
      "2015\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X_tfidf)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping new documents  into an already learned vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing a test classifier on new documents we need to keep the vocabulary **fixed**.\n",
    "\n",
    "We call `transform`, not `fit_transform` on the `sklearn` vectorizer.\n",
    "\n",
    "Words in new documents, will get mapped to the columns learned on the training set.\n",
    "New words not in the original vocabulary will be ignored.\n",
    "\n",
    "If we called `fit_transform` instead the indexing of words would be all **scrambled up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:00.630802Z",
     "start_time": "2019-09-19T16:59:37.198317Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test=countVectorizer.transform(test_documents[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the word counts of a few new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T17:00:00.640005Z",
     "start_time": "2019-09-19T17:00:00.631886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"doc\",\"word\"+\" \"*11,\"dim\",\"count\",sep=\"\\t\")\n",
    "for i1 in range(2):\n",
    "    for word in words:\n",
    "        document=X_test[i1]\n",
    "        dimension=countVectorizer.vocabulary_[word]\n",
    "        print(i1,f\"{word:15}\",dimension,document[0,dimension],sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a representation of a document as a stream of tokes (stems), we need to define the concept of a document distance metric.\n",
    "\n",
    "Usually in text processing the concept described is the normalized similarity $0<s(t_1,t_2)<1$, the translation to distance is simply\n",
    "\n",
    "$$\n",
    "    d(t_1,t_2) = 1-s(t_1,t_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, may different definitions of similarity are possible, we will consider three here:\n",
    "* Set intersection similarity.\n",
    "* vector of counts similarity.\n",
    "* TF-IDF (Term Frequency, Inverse Document Frequency) similarity.\n",
    "\n",
    "There are many more choices, and, as usual, which one works best depends on the  problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Intersection Similarity Measure\n",
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider our **bag of words** as just the set with the stems contained in document.\n",
    "\n",
    "Then document similarity between to documetns is just the normalized intersection of  the document's stem's sets.\n",
    "\n",
    "$$\n",
    "    S_{\\textrm{set}}(S_1,S_2)= \\frac{|S_1 \\cap S_2|}{\\sqrt{|S_1|\\cdot |S_2|}}\n",
    "$$\n",
    "where $|\\cdot|$ is the set cardinality (number of elements).\n",
    "\n",
    "In words: the *set similarity* of two documents is the ratio of the number of words in common to the geometric mean of the document's vocabulary length.\n",
    "\n",
    "This is the same as considering $S_i$ as a **one-hot-encoded** vector of words: each word in the vocabulary is a dimension, and a component of the vector is 1 if that word is present on the document, 0 otherwise.  \n",
    "\n",
    "With that interpretation\n",
    "$$\n",
    "    S_{\\textrm{set}}(S_1,S_2)= \\frac{S_1 \\cdot S_2}{\\sqrt{|S_1|\\cdot |S_2|}}=\\cos(S_1,S_2)\n",
    "$$\n",
    "where $\\cdot$ is the regular scalar product of vectors and $|\\cdot|$ is the vector norm (numerically identical to the set's cardinality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how documents from the same author are more similar to each other than to documents from a second author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Similarity Measure\n",
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include a bit more information on the **Bag of Words** model by keep track on how many times each word appear on a document\n",
    "$$\n",
    "    S_{\\textrm{count}} = \\frac{C_1 \\cdot C_2}{\\sqrt{|C_1|\\cdot |C_2|}}=\\cos(C_1,C_2)\n",
    "$$\n",
    "This is the same cosine similarity used on the set case but a  **count's feature vector** rather than been only 0 or 1, each dimension $w$ of document  $C$ contains the number times  word $w$ appears in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "`python` collections module has a [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) object that computes the counts for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, documents from the same author are closer to each other. But now the differences look more pronounced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "497px",
    "left": "0px",
    "right": "auto",
    "top": "107px",
    "width": "314px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
