{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10K_NeuralNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TbvuUrthAxJ7","colab_type":"text"},"source":["## Neural nets on labeled 10K dataset\n","\n","We tried a few different approaches to training a neural net to predict labels on the 10K dataset - \n","1. First, a plain vanilla neural net to classify excerpts as \"Relevant\" or \"No Disclosure\"\n","2. An LSTM model to do the same\n","3. A CNN, to eventually be used in transfer learning if successful enough\n","\n","Before this, we train word embeddings on the labeled dataset and vectorize the text using these embeddings.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qecTR8_jAH9Z","colab":{}},"source":["# Imports\n","\n","# Keras and Tensorflow\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D\n","from keras.optimizers import SGD\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","\n","import tensorflow as tf\n","\n","# Use scikit-learn for grid search, other basics\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n","from sklearn.metrics import mean_squared_error, recall_score, precision_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","\n","\n","# Progress tracker\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm_notebook\n","tqdm_notebook.pandas()\n","\n","# Import XGboost in case needed\n","import xgboost as xgb\n","\n","# Define recall function for Keras\n","recall = tf.keras.metrics.Recall(\n","    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xXxitjj9V03q","colab_type":"code","colab":{}},"source":["# More packages \n","\n","from sklearn.model_selection import train_test_split, GridSearchCV, \\\n","StratifiedKFold, cross_val_predict, \\\n","StratifiedShuffleSplit\n","from sklearn.feature_selection import chi2\n","from sklearn.metrics import roc_curve, \\\n","precision_recall_curve, auc, make_scorer, \\\n","recall_score, accuracy_score, precision_score, \\\n","confusion_matrix, classification_report\n","\n","#import gensim \n","from gensim.models import Word2Vec, KeyedVectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYbApg3-aruD","colab_type":"text"},"source":["### Train and apply word embeddings\n"," "]},{"cell_type":"code","metadata":{"id":"NOZ0vyqvBdKi","colab_type":"code","colab":{}},"source":["# Import json file\n","path = \"/Users/ishashah/Documents/DFG/dfg-humanrights0/from-sasb\"\n","os.chdir(path)\n","json = pd.read_json(\"di_hc_rel_train.json\")\n","json.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToOMCfFhBluE","colab_type":"code","colab":{}},"source":["# Import csv lookup\n","toplabel = pd.read_csv(\"disclosure_topic.csv\")\n","toplabel.columns = map(str.lower, toplabel.columns)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l66Y-drOBqjZ","colab_type":"code","colab":{}},"source":["# Create new label that flags labor only\n","toplabel[\"disclosure_islabor\"] = toplabel[\"disclosure_topic_name\"].str.contains(\"labor\", case = False)\n","json = pd.merge(json, toplabel, how = \"left\",\n","                on = \"disclosure_topic_id\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zu1lU0b-B-NW","colab_type":"code","colab":{}},"source":["# Check excerpts more closely\n","pd.options.display.max_colwidth = 500\n","json[\"excerpt\"].head()\n","\n","# Create a flag for these\n","json[\"relevant_islabor\"] = ((json[\"disclosure_islabor\"]) & (json[\"relevance_assessment\"] == \"Relevant\"))\n","json[\"relevant_islabor\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtGvBkaFCFBG","colab_type":"code","colab":{}},"source":["# Cleaning function\n","stopset = set(stopwords.words(\"english\"))\n","stemmer = PorterStemmer()\n","\n","def clean_text(in_text):\n","    # Remove line breaks\n","    text = in_text.replace(r'\\\\n', ' ')\n","    \n","    # Lowercase\n","    text = word_tokenize(re.sub('[^A-z ]+', '', text.lower()))\n","    \n","    # Remove stopwords, remove numbers and punctuation, stem\n","    text = [stemmer.stem(w) for w in text if w.isalpha() and w not in stopset]\n","    \n","    # Return joined version\n","    text = (\" \".join(text))\n","    \n","    return text\n","\n","# Apply cleaning function to json file text\n","json[\"clean_text\"] = json[\"excerpt\"].progress_apply(clean_text)\n","json.head()\n","\n","# Export csv of cleaned dataset\n","json.to_csv(\"json_clean.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vo7Jokr7V04D","colab_type":"code","colab":{},"outputId":"3ca5bb42-aad1-43a0-cdd8-ef2e77653a58"},"source":["# Create list of cleaned words in each excerpt\n","json[\"cleantext_list\"] = json[\"clean_text\"].apply(lambda x: ','.join(word_tokenize(x)))\n","sent = [row.split(',') for row in json[\"cleantext_list\"]]\n","\n","# Train on corpus\n","model = Word2Vec(sent, min_count=5, size= 300,workers=3, window =3, sg = 1)\n","\n","# Check vector size\n","model.vector_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"uWPLmhT8V05c","colab_type":"code","colab":{}},"source":["#  Save trained word embeddings\n","model.wv.save_word2vec_format('model.txt', binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQz_QVJkV05m","colab_type":"code","colab":{}},"source":["# Load trained word embeddings\n","model = KeyedVectors.load_word2vec_format('model.txt', binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOHHmKWcV05_","colab_type":"code","colab":{},"outputId":"c66a227b-dd2b-43d8-f8ac-6f55127dcec3"},"source":["# Vectorize using embeddings\n","import numpy as np\n","\n","def sent_vectorizer(sent, model):\n","    sent_vec =[]\n","    numw = 0\n","    for w in sent:\n","        try:\n","            if numw == 0:\n","                sent_vec = model[w]\n","            else:\n","                sent_vec = np.add(sent_vec, model[w])\n","            numw+=1\n","        except:\n","            pass\n","   \n","    return np.asarray(sent_vec) / numw\n","\n","\n","V=[]\n","\n","for sentence in sent:\n","    V.append(sent_vectorizer(sentence, model))   \n","    \n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/Users/ishashah/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  # Remove the CWD from sys.path while we load stuff.\n","/Users/ishashah/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9RNtgcp5V06P","colab_type":"code","colab":{}},"source":["# Get into dataframe\n","json2 = pd.DataFrame(V, index = json['excerpt_id'])\n","json2 = json2.merge(right=json[[\"excerpt_id\", \"relevance_assessment\"]], \n","         left_index=True, right_on=\"excerpt_id\")\n","\n","json2[[\"excerpt_id\", \"relevance_assessment\"]].head()\n","json2[\"relevance_assessment\"].value_counts()\n","\n","json.shape\n","json2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8Na9Ot9V06V","colab_type":"code","colab":{}},"source":["# Save vectorized version\n","json2.to_csv(\"json2_clean.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEonOUGvAxJ9","colab_type":"text"},"source":["### 1. Basic neural net with some gridsearch-based tuning\n","Summary: not too successful, accuracy only about 0.67 at most\n"]},{"cell_type":"code","metadata":{"id":"BMOFoCA21EeW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1599224615226,"user_tz":240,"elapsed":22996,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"5f71b9d0-a418-4965-c60a-06f55b5a6c95"},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oHVchsza0wfL","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z316LvwhAxKC","colab_type":"code","colab":{}},"source":["# Keep all output\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# # Set paths\n","# proxpath = \"/Users/ishashah/Documents/DFG/dfg-humanrights0/trim_corp\"\n","# os.chdir(proxpath)\n","\n","# Import csv of cleaned dataset\n","json = pd.read_csv(\"drive/My Drive/DFG Cost of Human Rights Violations/Datasets/smallcorp_1/json_clean.csv\")\n","\n","# Import csv of cleaned dataset with vectorization using word embeddings trained on 10Ks\n","json2 = pd.read_csv(\"drive/My Drive/DFG Cost of Human Rights Violations/Datasets/smallcorp_1/json2_clean.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AOnoCE4YAH92","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599185075867,"user_tz":240,"elapsed":227,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"f05ae9b3-6544-429c-f046-8ca7159403d6"},"source":["# Fix random seed\n","seed = 7\n","np.random.seed(seed)\n","\n","# Split X and y (using dataset vectorized w/ embeddings)\n","X = json2.iloc[:,0:300]\n","y = pd.DataFrame(pd.get_dummies(json2['relevance_assessment']))[\"Relevant\"]\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20626, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6Ld6vwpxAH94","colab":{}},"source":["# Simple trial run for using gridsearch cv\n","\n","def create_model(): \n","\n","\t# Create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=300, activation='relu'))\n","\tmodel.add(Dense(64, activation='relu'))\n","\tmodel.add(Dense(1, activation='sigmoid'))\n","    \n","\t# Compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\treturn model\n","\n","\n","model = KerasClassifier(build_fn=create_model, epochs=100, verbose=0) \n","\n","# Building a simple search grid that adjusts epochs\n","param_grid = dict(epochs=[10,20,30]) \n","grid = GridSearchCV(estimator=model, param_grid=param_grid)\n","grid_result = grid.fit(X, y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R3nVEw3QAH96","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599185317739,"user_tz":240,"elapsed":257,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"9c5fe9b3-d773-4acd-e400-b298fe018d0c"},"source":["# Print best number of epochs\n","# grid_result.cv_results_ for full results file\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best: 0.682970 using {'epochs': 30}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9uK16b2eAH9_","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"c54eee81-9578-493d-955e-235947e09a3f"},"source":["# Tuning different parameters\n","\n","def create_model():\n","\t# Create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=300, activation='relu'))\n","\tmodel.add(Dense(64, activation='relu'))\n","\tmodel.add(Dense(1, activation='sigmoid'))\n","\t# Compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    # Also tried SGD as optimizer but did not work as well\n","\treturn model\n","\n","# Call model function in KerasClassifier\n","model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n","\n","# Define the grid search parameters\n","param_grid = dict(epochs=[10,20,30,50],\n","                  learn_rate = [0.001, 0.01]) # add additional parameters\n","\n","grid = GridSearchCV(estimator=model, param_grid=param_grid)\n","grid_result = grid.fit(X, y)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best: 0.675503 using {'epochs': 30}\n","0.639529 (0.122126) with: {'epochs': 10}\n","0.650824 (0.101179) with: {'epochs': 20}\n","0.675503 (0.072801) with: {'epochs': 30}\n","0.664255 (0.092698) with: {'epochs': 50}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t0jAqkq1AxKa","colab_type":"text"},"source":["### 2. LSTM approach out of the box\n","Summary: Also not quite successful, max accuracy with 10 epochs only reaches about the same as basic net, 0.67\n","Note: did not use previous embeddings"]},{"cell_type":"code","metadata":{"id":"31A1zOnjeoxm","colab_type":"code","colab":{}},"source":["# Import packages for LSTM, in case\n","\n","import seaborn as sns\n","import re\n","import pandas as pd\n","import numpy as np\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Dropout\n","\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","STOPWORDS = set(stopwords.words('english'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAvloXMhV0_e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599185715202,"user_tz":240,"elapsed":1180,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"de73a6d3-5124-4bfa-d1a2-89d0c89c6f50"},"source":["\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 50000\n","# Max number of words in each excerpt.\n","MAX_SEQUENCE_LENGTH = 300\n","EMBEDDING_DIM = 300\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(json['clean_text'].values)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 24911 unique tokens.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zoeL7Ao1V0_k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599185722948,"user_tz":240,"elapsed":1546,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"fda66dba-8835-4019-9a30-826468237dc9"},"source":["# Original code for LSTM model, does not use pretrained embeddings\n","\n","X = tokenizer.texts_to_sequences(json['clean_text'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)\n","\n","Y = pd.get_dummies(json['relevance_assessment']).values\n","print('Shape of label tensor:', Y.shape)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)\n","\n","Y_train = Y_train.reshape((-1,1))\n","Y_test = Y_test.reshape((-1,1))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of data tensor: (20626, 300)\n","Shape of label tensor: (20626, 2)\n","(16500, 300) (16500, 2)\n","(4126, 300) (4126, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p5oNMYfWV0_t","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","recall = tf.keras.metrics.Recall(\n","    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n",")\n","from keras.layers import Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOQULXtXV0_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1599187452571,"user_tz":240,"elapsed":1366473,"user":{"displayName":"Isha Shah","photoUrl":"","userId":"08610669001507660647"}},"outputId":"b3bcffb9-f46e-4d13-d959-9656ee4c5197"},"source":["model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', \n","              optimizer='adam', \n","              metrics=['accuracy', recall])\n","\n","epochs = 10\n","batch_size = 64\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, \n","                    batch_size=batch_size,\n","                    validation_split=0.2,\n","                    callbacks=[EarlyStopping(monitor='accuracy', \n","                                             patience=3, \n","                                             min_delta=0.0001)])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","207/207 [==============================] - 339s 2s/step - loss: 0.6940 - accuracy: 0.4961 - recall_5: 0.5239 - val_loss: 0.6933 - val_accuracy: 0.5091 - val_recall_5: 0.6533\n","Epoch 2/10\n","207/207 [==============================] - 337s 2s/step - loss: 0.6822 - accuracy: 0.5654 - recall_5: 0.5865 - val_loss: 0.7025 - val_accuracy: 0.5003 - val_recall_5: 0.5267\n","Epoch 3/10\n","207/207 [==============================] - 339s 2s/step - loss: 0.6392 - accuracy: 0.6327 - recall_5: 0.6056 - val_loss: 0.7444 - val_accuracy: 0.4912 - val_recall_5: 0.4655\n","Epoch 4/10\n","207/207 [==============================] - 341s 2s/step - loss: 0.5667 - accuracy: 0.6977 - recall_5: 0.6885 - val_loss: 0.8120 - val_accuracy: 0.4870 - val_recall_5: 0.4091\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nYDkoq4bLyqM","colab_type":"code","colab":{}},"source":["model.save(f\"/content/drive/My Drive/DFG Cost of Human Rights Violations/Datasets/smallcorp_1/lstm1.model\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XvMiwqlfKtI","colab_type":"text"},"source":["### 3. Convolutional neural net\n","Summary: Ran into RAM issues on local machine, timeout on Colab\n"]},{"cell_type":"code","metadata":{"id":"x4txdQIvAxKi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"40439469-49c1-4936-cb00-1c6bd38f2196"},"source":["epochs = 20\n","embedding_dim = 300\n","maxlen = 300\n","\n","def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n","    model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n","    model.add(GlobalMaxPooling1D())\n","    model.add(Dense(12, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","# param_grid = dict(num_filters=[32, 64, 128],\n","#                   kernel_size=[3, 5, 7],\n","#                   vocab_size=[5000], \n","#                   embedding_dim=[300],\n","#                   maxlen=[300])\n","\n","\n","sentences = json['clean_text'].values\n","y = json['relevance_assessment'].values\n","\n","# Train-test split\n","sentences_train, sentences_test, y_train, y_test = train_test_split(\n","    sentences, y, test_size=0.20, random_state=seed)\n","\n","# Tokenize words\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(sentences_train)\n","X_train = tokenizer.texts_to_sequences(sentences_train)\n","X_test = tokenizer.texts_to_sequences(sentences_test)\n","\n","# Adding 1 because of reserved 0 index\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Pad sequences with zeros\n","X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n","\n","# Parameter grid for grid search\n","param_grid = dict(num_filters=[32, 64, 128],\n","                  kernel_size=[3, 5, 7],\n","                  vocab_size=[vocab_size],\n","                  embedding_dim=[embedding_dim],\n","                  maxlen=[maxlen])\n","model = KerasClassifier(build_fn=create_model,\n","                        epochs=epochs, batch_size=10,\n","                        verbose=False)\n","grid = GridSearchCV(estimator=model, param_grid=param_grid,\n","                    cv=5, verbose=1)\n","grid_result = grid.fit(X_train, y_train)\n","\n","# Evaluate testing set\n","test_accuracy = grid.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"d1ou2Iv898RH","colab_type":"code","colab":{}},"source":["model.save(f\"/content/drive/My Drive/DFG Cost of Human Rights Violations/Datasets/smallcorp_1/cnn1.model\")\n"],"execution_count":null,"outputs":[]}]}