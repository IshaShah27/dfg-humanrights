{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "#from DFG_Text_Features.ipynb import *\n",
    "#from E4525_ML import plots\n",
    "#plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=pd.read_csv(\"filenames.csv\",index_col=\"document_id\")\n",
    "data_dir='preprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from html import unescape\n",
    "\n",
    "# create a spaCy tokenizer\n",
    "#spacy.load('en')\n",
    "#lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def stem_tokenizer(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return [porter_stemmer.stem(token) for token in word_tokenize(text.lower())]\n",
    "\n",
    "# remove html entities from docs and\n",
    "# set everything to lowercase\n",
    "def my_preprocessor(text):\n",
    "    if (type(text) is not list):\n",
    "        text=text.split('\\n')\n",
    "    text=\" \".join(text).replace(\"\\n\",\" \").replace('\\u200b', '').replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "# tokenize the doc and lemmatize its tokens\n",
    "def my_tokenizer(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    punctuation=list(string.punctuation)\n",
    "    stop0=\" \".join(stopwords.words(\"english\")+punctuation)\n",
    "    stop_words=set(stem_tokenizer(stop0))\n",
    "    stem_list=stem_tokenizer(text)\n",
    "    used_list=[token for token in stem_list if token not in stop_words]\n",
    "    return used_list\n",
    "\n",
    "# create a dataframe from a word matrix\n",
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_features_filename=data_dir+ \"/\" + \"set_features.p\"\n",
    "set_features=pickle.load(open(set_features_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T08:50:21.465618Z",
     "start_time": "2019-09-27T08:50:21.435600Z"
    }
   },
   "outputs": [],
   "source": [
    "count_features_filename=data_dir+\"/\"+\"count_features.p\"\n",
    "count_features=pickle.load(open(count_features_filename,\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T08:50:21.476951Z",
     "start_time": "2019-09-27T08:50:21.467199Z"
    }
   },
   "outputs": [],
   "source": [
    "#count_test_features_filename=data_dir+\"/\"+\"count_test_features.p\"\n",
    "#count_test_features=pickle.load(open(count_test_features_filename,\"rb\"))\n",
    "#count_test_features=normalize(count_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T08:50:21.486800Z",
     "start_time": "2019-09-27T08:50:21.478596Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_features_filename=data_dir+\"/\"+\"tfidf_features.p\"\n",
    "tfidf_vectorizer_filename=data_dir+\"/\"+\"tfidf_vectorizer.p\"\n",
    "\n",
    "tfidf_features=pickle.load(open(tfidf_features_filename,\"rb\"))\n",
    "tfidf_vectorizer=pickle.load(open(tfidf_vectorizer_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T08:50:21.497191Z",
     "start_time": "2019-09-27T08:50:21.488411Z"
    }
   },
   "outputs": [],
   "source": [
    "#tfidf_test_features_filename=data_dir+\"/\"+\"tfidf_test_features.p\"\n",
    "#tfidf_test_features=pickle.load(open(tfidf_test_features_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x6532 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12418 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Drop In Model Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(tfidf_features)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
